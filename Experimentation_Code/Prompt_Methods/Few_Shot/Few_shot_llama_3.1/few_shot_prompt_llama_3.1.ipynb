{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bc8964f-17cf-4de9-bd01-7b970c4d14e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\anish\\anaconda3\\lib\\site-packages (23.3.1)\n",
      "Collecting pip\n",
      "  Using cached pip-25.2-py3-none-any.whl.metadata (4.7 kB)\n",
      "Using cached pip-25.2-py3-none-any.whl (1.8 MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: To modify pip, please run the following command:\n",
      "C:\\Users\\anish\\anaconda3\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: groq in c:\\users\\anish\\anaconda3\\lib\\site-packages (0.31.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\anish\\anaconda3\\lib\\site-packages (2.1.4)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from groq) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from groq) (1.8.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from groq) (0.27.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from groq) (2.8.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\anish\\anaconda3\\lib\\site-packages (from groq) (1.3.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from groq) (4.12.2)\n",
      "Requirement already satisfied: numpy<2,>=1.23.2 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from anyio<5,>=3.5.0->groq) (3.4)\n",
      "Requirement already satisfied: certifi in c:\\users\\anish\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->groq) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\anish\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->groq) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (2.20.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install groq pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7dc95295-e756-401e-9529-47c87be59571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== PATHS / MODEL CONFIG ====\n",
    "EVAL_SELECTION_CSV = \"medquad_selected_questions.csv\"  # <- same questions file you used earlier\n",
    "ANSWER_FIELD       = \"answer\"  # if your selection CSV has 'gold', we'll auto-detect below\n",
    "\n",
    "# Groq model for generation (few-shot answering)\n",
    "GROQ_API_KEY = \"\"   # <-- put your key \n",
    "GEN_MODEL    = \"llama-3.1-8b-instant\"    # or \"llama-3.1-8b-instant\" if you prefer free/cheaper\n",
    "\n",
    "# Generation knobs (keep fixed across runs for fair comparison)\n",
    "TEMPERATURE = 0.0\n",
    "MAX_TOKENS  = 256\n",
    "TOP_P       = 1.0\n",
    "\n",
    "# Ablation settings (how many examples to prepend)\n",
    "FEWSHOT_LIST = [2, 3, 5]   # run all of these\n",
    "PRINT_PROMPTS_FOR_FEWSHOT = False  # True = print the full prompt sent to the model\n",
    "\n",
    "# Where to save outputs\n",
    "RESULTS_DIR = \".\"\n",
    "\n",
    "# (Optional) run the LLM-as-judge scoring after each ablation run\n",
    "RUN_EVAL = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cff10415-42d5-493f-9d79-e6f447245630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Few-shot evaluation questions: 3\n",
      "              Do you have information about X-Rays\n",
      "What are the symptoms of Alpha-ketoglutarate de...\n",
      "What are the treatments for GLUT1 deficiency sy...\n"
     ]
    }
   ],
   "source": [
    "import os, time, json, re, pandas as pd\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "# Load the SAME questions file you used before\n",
    "sel = pd.read_csv(EVAL_SELECTION_CSV)\n",
    "\n",
    "# figure out which column in the selection file holds the gold text\n",
    "gold_col = \"gold\" if \"gold\" in sel.columns else (ANSWER_FIELD if ANSWER_FIELD in sel.columns else None)\n",
    "assert gold_col is not None, f\"Selection file must contain either 'gold' or '{ANSWER_FIELD}'\"\n",
    "\n",
    "# keep the rows you want (you can change head(3) to use more)\n",
    "eval_df = sel[[\"question\", gold_col]].copy().head(3)\n",
    "eval_df.columns = [\"question\", \"gold\"]  # internal rename only (we're not saving this CSV)\n",
    "\n",
    "print(\"Few-shot evaluation questions:\", len(eval_df))\n",
    "print(eval_df[\"question\"].to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "918d467c-5df5-4b14-9ead-b873620720af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "\n",
    "client = Groq(api_key=GROQ_API_KEY)\n",
    "\n",
    "def chat_messages(model: str, messages: List[Dict], temperature: float = 0.0, max_tokens: int = 256, top_p: float = 1.0) -> str:\n",
    "    \"\"\"\n",
    "    Thin wrapper around Groq chat completions. Returns the text content.\n",
    "    \"\"\"\n",
    "    r = client.chat.completions.create(\n",
    "        model=model, temperature=temperature, max_tokens=max_tokens, top_p=top_p,\n",
    "        messages=messages\n",
    "    )\n",
    "    return r.choices[0].message.content.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b8aaa45-8f0e-4a14-b3b6-d2fbd4343495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Curated few-shot exemplars (medical, generic, not the same as your eval questions) ---\n",
    "# Feel free to edit/add more examples here; the builder will pick the first k you request.\n",
    "CURATED_FEWSHOTS: List[Tuple[str, str]] = [\n",
    "    (\n",
    "        \"What are common side effects of the influenza vaccine?\",\n",
    "        \"Most side effects are mild and short-lived, such as soreness at the injection site, low-grade fever, fatigue, or headache. \"\n",
    "        \"Severe allergic reactions are rare; anyone with a history of anaphylaxis to a vaccine component should discuss risks with a clinician.\"\n",
    "    ),\n",
    "    (\n",
    "        \"How is mild dehydration treated at home?\",\n",
    "        \"Oral rehydration with water or oral rehydration solutions is first-line. Sip small amounts frequently, avoid alcohol and caffeine, \"\n",
    "        \"and address the underlying cause (e.g., gastrointestinal losses). Seek care if symptoms persist or worsen.\"\n",
    "    ),\n",
    "    (\n",
    "        \"Who should be screened for hypertension?\",\n",
    "        \"All adults should have periodic blood pressure screening. Screening is especially important for individuals with risk factors such as \"\n",
    "        \"obesity, diabetes, kidney disease, or a family history of hypertension.\"\n",
    "    ),\n",
    "    (\n",
    "        \"What are red-flag symptoms of chest pain?\",\n",
    "        \"Red flags include pressure-like pain radiating to the arm/jaw, shortness of breath, diaphoresis, syncope, or hemodynamic instability. \"\n",
    "        \"Immediate evaluation is warranted to rule out acute coronary syndrome or other emergent causes.\"\n",
    "    ),\n",
    "    (\n",
    "        \"When are antibiotics indicated for acute bronchitis?\",\n",
    "        \"Most cases are viral; antibiotics are generally not indicated unless there is strong suspicion of bacterial infection, \"\n",
    "        \"high-risk comorbidities, or evidence of pneumonia. Symptomatic care is usually sufficient.\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "def build_few_shot_messages(question: str, k: int = 3, print_prompt: bool = False) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Build a few-shot prompt of k exemplars + your question.\n",
    "    Prints the full prompt if print_prompt=True (good for debugging/learning).\n",
    "    \"\"\"\n",
    "    k = max(0, min(k, len(CURATED_FEWSHOTS)))\n",
    "    exemplars = CURATED_FEWSHOTS[:k]\n",
    "\n",
    "    # System message keeps the model focused and concise\n",
    "    system_msg = (\n",
    "        \"You are a concise, evidence-focused medical assistant. \"\n",
    "        \"Answer briefly (2–4 sentences) and avoid speculation. If unsure, say you don't know.\"\n",
    "    )\n",
    "\n",
    "    # Build a readable 'Examples' block (Q/A pairs)\n",
    "    examples_block = []\n",
    "    for i, (q_ex, a_ex) in enumerate(exemplars, start=1):\n",
    "        examples_block.append(f\"Example {i} — Question: {q_ex}\\nExample {i} — Answer: {a_ex}\")\n",
    "    examples_text = \"\\n\\n\".join(examples_block) if examples_block else \"(No examples)\"\n",
    "\n",
    "    user_msg = (\n",
    "        f\"Use the examples below as a style and reasoning guide.\\n\\n\"\n",
    "        f\"{examples_text}\\n\\n\"\n",
    "        f\"Now answer the new question:\\nQuestion: {question}\\n\\n\"\n",
    "        f\"Instructions: Provide a factual, succinct answer in 2–4 sentences. \"\n",
    "        f\"If information is insufficient, say 'I don't know.'\"\n",
    "    )\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_msg},\n",
    "        {\"role\": \"user\",   \"content\": user_msg},\n",
    "    ]\n",
    "\n",
    "    if print_prompt:\n",
    "        print(\"\\n\" + \"=\"*88)\n",
    "        print(\"[FEW-SHOT PROMPT]\")\n",
    "        print(\"\\n[SYSTEM]\\n\" + system_msg)\n",
    "        print(\"\\n[USER]\\n\" + user_msg)\n",
    "        print(\"=\"*88)\n",
    "\n",
    "    return messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48e0c434-ba43-4e83-8c10-3ce950ebb7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "def normalize_text(t: str) -> str:\n",
    "    t = (t or \"\").strip()\n",
    "    t = unicodedata.normalize(\"NFKC\", t)\n",
    "    t = re.sub(r\"\\s+\", \" \", t)\n",
    "    return t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8ab2471-74b8-4470-9a17-a6b0d64f5ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def run_few_shot(n_shots: int, print_prompts: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Runs few-shot prompting over eval_df with n_shots exemplars.\n",
    "    Returns the results DataFrame and saves a CSV tagged with n_shots.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for _, r in eval_df.iterrows():\n",
    "        q   = normalize_text(str(r[\"question\"]))\n",
    "        gold= normalize_text(str(r[\"gold\"]))\n",
    "\n",
    "        messages = build_few_shot_messages(q, k=n_shots, print_prompt=print_prompts)\n",
    "        ans = chat_messages(GEN_MODEL, messages, temperature=TEMPERATURE, max_tokens=MAX_TOKENS, top_p=TOP_P)\n",
    "\n",
    "        rows.append({\n",
    "            \"question\": q,\n",
    "            \"gold\": gold,\n",
    "            \"strategy\": f\"few-shot-{n_shots}\",\n",
    "            \"answer\": normalize_text(ans),\n",
    "        })\n",
    "\n",
    "    df_out = pd.DataFrame(rows)\n",
    "    out_csv = f\"{RESULTS_DIR}/medquad_few_shot_answers_n{n_shots}.csv\"\n",
    "    df_out.to_csv(out_csv, index=False)\n",
    "    print(f\"[FEW-SHOT] Saved answers to: {out_csv}  (rows={len(df_out)})\")\n",
    "    return df_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "148c61f1-0ff4-486d-a075-6cc1c516a136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === LLM-as-judge for: Faithfulness, Hallucination, Correctness, Relevance ===\n",
    "# Uses Groq judge model (fast & cheap) separate from your generator model.\n",
    "\n",
    "import re, pandas as pd\n",
    "from typing import List\n",
    "\n",
    "JUDGE_MODEL = \"llama-3.3-70b-versatile\"   # <- LLM-as-judge\n",
    "JUDGE_TEMPERATURE = 0.0\n",
    "JUDGE_MAX_TOKENS = 64\n",
    "JUDGE_TOP_P = 1.0\n",
    "\n",
    "def _extract_float(txt: str) -> float:\n",
    "    m = re.search(r\"\\d*\\.?\\d+(?:[eE][-+]?\\d+)?\", txt or \"\")\n",
    "    try: x = float(m.group(0)) if m else 0.0\n",
    "    except: x = 0.0\n",
    "    return max(0.0, min(1.0, x))\n",
    "\n",
    "def chat_judge(system_prompt: str, user_prompt: str, model: str = JUDGE_MODEL) -> str:\n",
    "    r = client.chat.completions.create(\n",
    "        model=model, temperature=JUDGE_TEMPERATURE, max_tokens=JUDGE_MAX_TOKENS, top_p=JUDGE_TOP_P,\n",
    "        messages=[{\"role\":\"system\",\"content\":system_prompt},\n",
    "                  {\"role\":\"user\",\"content\":user_prompt}]\n",
    "    )\n",
    "    return r.choices[0].message.content.strip()\n",
    "\n",
    "def entail_prob(premise: str, claim: str) -> float:\n",
    "    \"\"\"Return P(premise entails claim) in [0,1].\"\"\"\n",
    "    sys = (\"You are an evaluator. Given a PREMISE (evidence) and a CLAIM (one sentence), \"\n",
    "           \"return ONLY a number in [0,1] = probability that PREMISE ENTAILS CLAIM.\")\n",
    "    usr = f\"PREMISE:\\n{premise}\\n\\nCLAIM:\\n{claim}\\n\\nOutput only a number in [0,1].\"\n",
    "    return _extract_float(chat_judge(sys, usr))\n",
    "\n",
    "def split_sentences(t: str) -> List[str]:\n",
    "    t = (t or \"\").strip()\n",
    "    # light splitter—good enough for scoring short answers\n",
    "    return re.split(r'(?<=[.!?])\\s+', t) if t else []\n",
    "\n",
    "def faithfulness_verbose(answer: str, gold_reference: str, thresh: float = 0.5):\n",
    "    \"\"\"\n",
    "    Treat the gold answer as the 'evidence'. For each sentence in the model answer,\n",
    "    compute entailment from gold->sentence. Faithfulness = fraction supported (>= thresh).\n",
    "    \"\"\"\n",
    "    sents = [s.strip() for s in split_sentences(answer) if s.strip()]\n",
    "    if not sents:\n",
    "        return 0.0, 1.0, pd.DataFrame(columns=[\"sentence\",\"best_entail_prob\",\"supported\"])\n",
    "    rows, supported = [], 0\n",
    "    for s in sents:\n",
    "        p = entail_prob(gold_reference, s)\n",
    "        sup = p >= thresh\n",
    "        supported += int(sup)\n",
    "        rows.append({\"sentence\": s, \"best_entail_prob\": round(p,3), \"supported\": sup})\n",
    "    faith = supported / len(sents)\n",
    "    halluc = 1.0 - faith\n",
    "    return faith, halluc, pd.DataFrame(rows)\n",
    "\n",
    "def answer_correctness_llm(gold_answer: str, model_answer: str) -> float:\n",
    "    \"\"\"Bidirectional: 0.5*(E(gold->answer)+E(answer->gold)).\"\"\"\n",
    "    e1 = entail_prob(gold_answer, model_answer)\n",
    "    e2 = entail_prob(model_answer, gold_answer)\n",
    "    return 0.5 * (e1 + e2)\n",
    "\n",
    "def answer_relevance(q: str, a: str) -> float:\n",
    "    sys = (\n",
    "        \"You are an evaluator. Rate how well the ANSWER addresses the QUESTION.\\n\"\n",
    "        \"- 1.0 = Directly answers, accurate and focused.\\n\"\n",
    "        \"- 0.7 = Mostly answers with minor gaps/irrelevance.\\n\"\n",
    "        \"- 0.4 = Partial answer; noticeable gaps or off-topic parts.\\n\"\n",
    "        \"- 0.0 = Does not answer or off-topic.\\n\"\n",
    "        \"Return ONLY a number in [0,1].\"\n",
    "    )\n",
    "    usr = f\"QUESTION:\\n{q}\\n\\nANSWER:\\n{a}\\n\\nScore:\"\n",
    "    return _extract_float(chat_judge(sys, usr))\n",
    "\n",
    "def band(x: float) -> str:\n",
    "    return \"Excellent\" if x>=0.90 else \"Good\" if x>=0.75 else \"Borderline\" if x>=0.60 else \"Poor\"\n",
    "\n",
    "def score_all_metrics(df_answers: pd.DataFrame, faith_thresh: float = 0.5) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds: faithfulness, hallucination_rate, answer_relevance, answer_correctness (+ bands).\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for _, r in df_answers.iterrows():\n",
    "        q, a, g = r[\"question\"], r[\"answer\"], r[\"gold\"]\n",
    "        faith, halluc, _df = faithfulness_verbose(a, g, thresh=faith_thresh)\n",
    "        relev = answer_relevance(q, a)\n",
    "        corr  = answer_correctness_llm(g, a)\n",
    "        rows.append({\n",
    "            \"question\": q,\n",
    "            \"strategy\": r.get(\"strategy\",\"\"),\n",
    "            \"faithfulness\": round(faith, 3),\n",
    "            \"hallucination_rate\": round(halluc, 3),\n",
    "            \"answer_relevance\": round(relev, 3),\n",
    "            \"answer_correctness\": round(corr, 3),\n",
    "            \"faith_band\": band(faith),\n",
    "            \"relevance_band\": band(relev),\n",
    "            \"correctness_band\": band(corr),\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fab139b8-52e7-4741-97cb-4f69335dc1de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================================================================\n",
      "[ABLATION] FEW-SHOT with N=2\n",
      "========================================================================================\n",
      "[FEW-SHOT] Saved answers to: ./medquad_few_shot_answers_n2.csv  (rows=3)\n",
      "[EVAL] Saved per-item scores to: ./medquad_few_shot_scores_n2.csv\n",
      "\n",
      "========================================================================================\n",
      "[ABLATION] FEW-SHOT with N=3\n",
      "========================================================================================\n",
      "[FEW-SHOT] Saved answers to: ./medquad_few_shot_answers_n3.csv  (rows=3)\n",
      "[EVAL] Saved per-item scores to: ./medquad_few_shot_scores_n3.csv\n",
      "\n",
      "========================================================================================\n",
      "[ABLATION] FEW-SHOT with N=5\n",
      "========================================================================================\n",
      "[FEW-SHOT] Saved answers to: ./medquad_few_shot_answers_n5.csv  (rows=3)\n",
      "[EVAL] Saved per-item scores to: ./medquad_few_shot_scores_n5.csv\n",
      "\n",
      "=== FEW-SHOT ABLATION SUMMARY (higher is better) ===\n",
      " n_shots  faithfulness_avg  hallucination_rate_avg  answer_relevance_avg  answer_correctness_avg\n",
      "       5          0.616667                0.383333              0.133333                0.033333\n",
      "       2          0.555667                0.444333              0.566667                0.575000\n",
      "       3          0.166667                0.833333              0.000000                0.166667\n",
      "Saved summary to: ./medquad_few_shot_ablation_summary_full_metrics.csv\n"
     ]
    }
   ],
   "source": [
    "# === Few-shot ablation over N in FEWSHOT_LIST, with full metric coverage ===\n",
    "all_runs = {}\n",
    "all_scores = []\n",
    "\n",
    "for N in FEWSHOT_LIST:\n",
    "    print(\"\\n\" + \"=\"*88)\n",
    "    print(f\"[ABLATION] FEW-SHOT with N={N}\")\n",
    "    print(\"=\"*88)\n",
    "\n",
    "    dfN = run_few_shot(N, print_prompts=PRINT_PROMPTS_FOR_FEWSHOT)   # uses your generator\n",
    "    all_runs[N] = dfN\n",
    "\n",
    "    # Score all four metrics\n",
    "    scored = score_all_metrics(dfN, faith_thresh=0.5)\n",
    "    scores_csv = f\"{RESULTS_DIR}/medquad_few_shot_scores_n{N}.csv\"\n",
    "    scored.to_csv(scores_csv, index=False)\n",
    "    print(f\"[EVAL] Saved per-item scores to: {scores_csv}\")\n",
    "\n",
    "    # Aggregate for this N\n",
    "    aggN = {\n",
    "        \"n_shots\": N,\n",
    "        \"faithfulness_avg\": float(scored[\"faithfulness\"].mean()),\n",
    "        \"hallucination_rate_avg\": float(scored[\"hallucination_rate\"].mean()),\n",
    "        \"answer_relevance_avg\": float(scored[\"answer_relevance\"].mean()),\n",
    "        \"answer_correctness_avg\": float(scored[\"answer_correctness\"].mean()),\n",
    "    }\n",
    "    all_scores.append(aggN)\n",
    "\n",
    "# Final comparison table\n",
    "agg_df = pd.DataFrame(all_scores).sort_values(by=\"faithfulness_avg\", ascending=False)\n",
    "summary_csv = f\"{RESULTS_DIR}/medquad_few_shot_ablation_summary_full_metrics.csv\"\n",
    "agg_df.to_csv(summary_csv, index=False)\n",
    "\n",
    "print(\"\\n=== FEW-SHOT ABLATION SUMMARY (higher is better) ===\")\n",
    "print(agg_df.to_string(index=False))\n",
    "print(\"Saved summary to:\", summary_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb70d0c-66e6-4bd7-88ef-9600b0f6046c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
