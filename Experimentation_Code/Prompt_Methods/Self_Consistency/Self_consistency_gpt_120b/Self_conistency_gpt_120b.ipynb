{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec3997de-7f98-4e05-bb49-8f58aa6c5444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\anish\\anaconda3\\lib\\site-packages (23.3.1)\n",
      "Collecting pip\n",
      "  Using cached pip-25.2-py3-none-any.whl.metadata (4.7 kB)\n",
      "Using cached pip-25.2-py3-none-any.whl (1.8 MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: To modify pip, please run the following command:\n",
      "C:\\Users\\anish\\anaconda3\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\anish\\anaconda3\\lib\\site-packages (2.1.4)\n",
      "Requirement already satisfied: groq in c:\\users\\anish\\anaconda3\\lib\\site-packages (0.31.0)\n",
      "Requirement already satisfied: numpy<2,>=1.23.2 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from groq) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from groq) (1.8.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from groq) (0.27.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from groq) (2.8.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\anish\\anaconda3\\lib\\site-packages (from groq) (1.3.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from groq) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from anyio<5,>=3.5.0->groq) (3.4)\n",
      "Requirement already satisfied: certifi in c:\\users\\anish\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->groq) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\anish\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->groq) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (2.20.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install pandas groq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7673d8d-149e-418a-8315-46c23400f7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Inputs (same style you used elsewhere) ===\n",
    "EVAL_SELECTION_CSV = \"medquad_selected_questions.csv\"\n",
    "ANSWER_FIELD       = \"answer\"   # if your CSV has 'gold', we auto-detect below\n",
    "\n",
    "# === Model / API ===\n",
    "from groq import Groq\n",
    "GROQ_API_KEY = \"\"\n",
    "GEN_MODEL    = \"openai/gpt-oss-120b\"     # generator for CoT\n",
    "JUDGE_MODEL  = \"llama-3.1-8b-instant\"     # judge for scoring & tie-breaks (you can switch)\n",
    "\n",
    "# === Self-consistency settings ===\n",
    "K_LIST        = [3, 5]     # ablation: how many CoT samples per question\n",
    "TEMP          = 0.7        # higher temp -> diverse samples\n",
    "TOP_P         = 1.0\n",
    "MAX_TOKENS    = 512\n",
    "PRINT_PROMPTS = True       # print the exact CoT prompt (first sample per question)\n",
    "\n",
    "# === Output ===\n",
    "RESULTS_DIR = \".\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a606a606-fea5-45aa-8bcd-ac0da0135aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load questions (uses your same CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cdbfec9-609d-4d59-a9b7-93b832578b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Questions loaded: 3\n",
      "              Do you have information about X-Rays\n",
      "What are the symptoms of Alpha-ketoglutarate de...\n",
      "What are the treatments for GLUT1 deficiency sy...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sel = pd.read_csv(EVAL_SELECTION_CSV)\n",
    "gold_col = \"gold\" if \"gold\" in sel.columns else (ANSWER_FIELD if ANSWER_FIELD in sel.columns else None)\n",
    "assert gold_col is not None, f\"Selection file must contain either 'gold' or '{ANSWER_FIELD}'\"\n",
    "\n",
    "eval_df = sel[[\"question\", gold_col]].copy()\n",
    "eval_df.columns = [\"question\", \"gold\"]     # internal convenience\n",
    "print(\"Questions loaded:\", len(eval_df))\n",
    "print(eval_df[\"question\"].head(10).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0846c025-fc6f-43fe-ad08-f19d75e2655e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Groq client + CoT prompt builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744d1226-1174-4794-86d2-da39a21ce650",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d1f4f93-b114-4d32-8740-a8cc6c033c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "import random\n",
    "from groq import Groq\n",
    "\n",
    "client = Groq(api_key=GROQ_API_KEY)\n",
    "\n",
    "def chat_messages(model: str, messages: List[Dict], temperature: float, max_tokens: int, top_p: float, seed: int | None = None) -> str:\n",
    "    \"\"\"Generic chat call; returns text only.\"\"\"\n",
    "    r = client.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "        top_p=top_p,\n",
    "        messages=messages,\n",
    "        **({\"seed\": seed} if seed is not None else {})\n",
    "    )\n",
    "    return r.choices[0].message.content.strip()\n",
    "\n",
    "def build_cot_messages(question: str, print_prompt: bool = False) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Static CoT template (no examples). Model is nudged to produce Reasoning bullets + Final Answer.\n",
    "    \"\"\"\n",
    "    system_msg = (\n",
    "        \"You are a concise, evidence-focused medical assistant. \"\n",
    "        \"Reason step-by-step using brief bullet points, then provide a final answer. \"\n",
    "        \"If unsure, say you don't know.\"\n",
    "    )\n",
    "    user_msg = (\n",
    "        \"Question: \" + question + \"\\n\\n\"\n",
    "        \"Follow this exact format:\\n\"\n",
    "        \"Reasoning:\\n\"\n",
    "        \"- bullet 1\\n- bullet 2\\n- bullet 3\\n\"\n",
    "        \"Final Answer: <one concise sentence>\\n\\n\"\n",
    "        \"Be brief and avoid speculation.\"\n",
    "    )\n",
    "    messages = [{\"role\":\"system\",\"content\":system_msg},\n",
    "                {\"role\":\"user\",\"content\":user_msg}]\n",
    "    if PRINT_PROMPTS and print_prompt:\n",
    "        print(\"\\n\" + \"=\"*88)\n",
    "        print(\"[SELF-CONSISTENCY / CoT PROMPT]\")\n",
    "        print(\"\\n[SYSTEM]\\n\" + system_msg)\n",
    "        print(\"\\n[USER]\\n\" + user_msg)\n",
    "        print(\"=\"*88)\n",
    "    return messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8df3c911-4d95-4d0d-bf5f-f87dfb56a9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Normalization + CoT parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38e2b055-ed1f-43a8-8923-3cf465ff8aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, unicodedata\n",
    "\n",
    "def normalize_text(t: str) -> str:\n",
    "    t = (t or \"\").strip()\n",
    "    t = unicodedata.normalize(\"NFKC\", t)\n",
    "    t = re.sub(r\"\\s+\", \" \", t)\n",
    "    return t\n",
    "\n",
    "def parse_cot(text: str):\n",
    "    \"\"\"\n",
    "    Return (reasoning, final, raw_cleaned).\n",
    "    We look for 'Final Answer:'; fallback to last sentence if missing.\n",
    "    \"\"\"\n",
    "    raw = normalize_text(text)\n",
    "    m = re.search(r\"(?:Final\\s*Answer\\s*:\\s*)(.*)$\", raw, flags=re.IGNORECASE|re.DOTALL)\n",
    "    if m:\n",
    "        final = m.group(1).strip()\n",
    "        reasoning = raw[:m.start()].strip()\n",
    "        return reasoning, final, raw\n",
    "    # fallback\n",
    "    parts = re.split(r'(?<=[.!?])\\s+', raw)\n",
    "    final = parts[-1].strip() if parts else raw\n",
    "    reasoning = raw[:max(0, raw.rfind(final))].strip()\n",
    "    return reasoning, final, raw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1842610-36de-474a-9cc6-c544efb90c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate k CoT samples per question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "626a928c-09ac-4f7e-9cd6-d38ceaa5da30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def generate_cot_samples(question: str, k: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Produce k independent CoT answers (diverse via temperature and different seeds).\n",
    "    Returns DataFrame: sample_id, cot_reasoning, cot_final, raw\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for i in range(k):\n",
    "        msgs = build_cot_messages(question, print_prompt=(i == 0))\n",
    "        seed = random.randint(1, 10_000_000)    # encourage diversity\n",
    "        out  = chat_messages(GEN_MODEL, msgs, temperature=TEMP, max_tokens=MAX_TOKENS, top_p=TOP_P, seed=seed)\n",
    "        reasoning, final, raw = parse_cot(out)\n",
    "        rows.append({\"sample_id\": i, \"cot_reasoning\": reasoning, \"cot_final\": final, \"raw\": raw})\n",
    "    return pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28851c70-ae2a-4c99-add6-6366f0024f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Pure text majority vote (no embeddings) + LLM tie-break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35a149eb-2b95-42f9-91a6-6aed8fb00f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def canonicalize(ans: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize free-text for fair majority voting (no embeddings):\n",
    "    - lowercase\n",
    "    - strip 'Final Answer:' boilerplate\n",
    "    - keep alphanum, %, /, -, .\n",
    "    - collapse whitespace\n",
    "    \"\"\"\n",
    "    s = (ans or \"\").lower()\n",
    "    s = re.sub(r\"final\\s*answer\\s*:\\s*\", \" \", s)\n",
    "    s = re.sub(r\"[^a-z0-9\\s.%/-]\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def majority_consensus(finals: list[str], min_votes: int = 2):\n",
    "    \"\"\"\n",
    "    Returns (best_index, diagnostics). If no majority >= min_votes, returns None to trigger tie-break.\n",
    "    \"\"\"\n",
    "    canon = [canonicalize(a) for a in finals]\n",
    "    counts = Counter(canon)\n",
    "    best_canon, votes = counts.most_common(1)[0]\n",
    "    if votes >= min_votes:\n",
    "        best_idx = next(i for i, c in enumerate(canon) if c == best_canon)\n",
    "        return best_idx, {\"method\": \"majority\", \"votes\": int(votes), \"canonical\": best_canon}\n",
    "    return None, {\"method\": \"no_majority\", \"votes\": int(votes)}\n",
    "\n",
    "# ---- LLM-as-judge tie-breaker (no embeddings) ----\n",
    "# We pick the candidate that has the highest average bidirectional entailment to others.\n",
    "JUDGE_TEMPERATURE = 0.0\n",
    "JUDGE_MAX_TOKENS  = 32\n",
    "JUDGE_TOP_P       = 1.0\n",
    "MAX_RETRIES       = 2\n",
    "PRINT_JUDGE_PROMPTS = False\n",
    "JUDGE_CALLS = 0  # simple cost log\n",
    "\n",
    "def _parse_score_strict(txt: str) -> float:\n",
    "    s = (txt or \"\").strip()\n",
    "    m = re.search(r'(?<![\\d.])(0(?:\\.\\d+)?|1(?:\\.0+)?|\\d\\.\\d+)(?![\\d.])', s)\n",
    "    try:\n",
    "        v = float(m.group(1)) if m else 0.0\n",
    "    except:\n",
    "        v = 0.0\n",
    "    return max(0.0, min(1.0, v))\n",
    "\n",
    "def judge_number(system_prompt: str, user_prompt: str) -> float:\n",
    "    \"\"\"Ask the judge for a bare number in [0,1] with retries.\"\"\"\n",
    "    global JUDGE_CALLS\n",
    "    for attempt in range(MAX_RETRIES):\n",
    "        if PRINT_JUDGE_PROMPTS:\n",
    "            print(\"\\n[SYSTEM]\\n\", system_prompt, \"\\n[USER]\\n\", user_prompt)\n",
    "        r = client.chat.completions.create(\n",
    "            model=JUDGE_MODEL,\n",
    "            temperature=JUDGE_TEMPERATURE,\n",
    "            max_tokens=JUDGE_MAX_TOKENS,\n",
    "            top_p=JUDGE_TOP_P,\n",
    "            messages=[{\"role\":\"system\",\"content\":system_prompt},\n",
    "                      {\"role\":\"user\",\"content\":user_prompt}]\n",
    "        )\n",
    "        JUDGE_CALLS += 1\n",
    "        val = _parse_score_strict(r.choices[0].message.content)\n",
    "        if 0.0 <= val <= 1.0:\n",
    "            return val\n",
    "    return 0.0\n",
    "\n",
    "def entail_prob(premise: str, claim: str) -> float:\n",
    "    sys = \"Return ONLY a bare number in [0,1] = P(premise entails claim). No words.\"\n",
    "    usr = f\"premise:\\n{premise}\\n\\nclaim:\\n{claim}\\n\\nnumber:\"\n",
    "    return judge_number(sys, usr)\n",
    "\n",
    "def tie_break_by_entailment(finals: list[str]) -> int:\n",
    "    \"\"\"\n",
    "    Pick the answer that best 'agrees' with the others:\n",
    "    score(i) = mean_j 0.5*[E(i->j) + E(j->i)]\n",
    "    \"\"\"\n",
    "    n = len(finals)\n",
    "    if n == 1: \n",
    "        return 0\n",
    "    scores = []\n",
    "    for i in range(n):\n",
    "        s = 0.0\n",
    "        for j in range(n):\n",
    "            if i == j: \n",
    "                continue\n",
    "            eij = entail_prob(finals[i], finals[j])\n",
    "            eji = entail_prob(finals[j], finals[i])\n",
    "            s += 0.5 * (eij + eji)\n",
    "        scores.append(s / max(1, n-1))\n",
    "    return int(max(range(n), key=lambda i: scores[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d1c4dd5-df22-446c-a980-e7cb921305fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Metrics (Faithfulness soft+hard, Hallucination, Relevance, Correctness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60db25f0-9b04-4c95-94ac-c7d4f997ece1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, pandas as pd\n",
    "\n",
    "def split_sents(t: str):\n",
    "    t = (t or \"\").strip()\n",
    "    sents = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', t) if s.strip()]\n",
    "    return sents if sents else ([t] if t else [])\n",
    "\n",
    "def faithfulness_metrics(answer: str, gold_reference: str, hard_thresh_list=(0.5, 0.8)):\n",
    "    sents = split_sents(answer)\n",
    "    if not sents:\n",
    "        return [], 0.0, {th: 0.0 for th in hard_thresh_list}\n",
    "    probs = [entail_prob(gold_reference, s) for s in sents]\n",
    "    soft = sum(probs) / len(probs)\n",
    "    hard = {th: sum(p >= th for p in probs) / len(probs) for th in hard_thresh_list}\n",
    "    return probs, soft, hard\n",
    "\n",
    "def answer_correctness_llm(gold_answer: str, model_answer: str) -> float:\n",
    "    e1 = entail_prob(gold_answer, model_answer)\n",
    "    e2 = entail_prob(model_answer, gold_answer)\n",
    "    return 0.5 * (e1 + e2)\n",
    "\n",
    "def answer_relevance(q: str, a: str) -> float:\n",
    "    sys = \"Return ONLY a bare number in [0,1] = how well ANSWER addresses QUESTION. No words.\"\n",
    "    usr = f\"QUESTION:\\n{q}\\n\\nANSWER:\\n{a}\\n\\nnumber:\"\n",
    "    return judge_number(sys, usr)\n",
    "\n",
    "def band(x: float) -> str:\n",
    "    return \"Excellent\" if x>=0.90 else \"Good\" if x>=0.75 else \"Borderline\" if x>=0.60 else \"Poor\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3bf2d61a-360b-4409-9864-833a7fd296e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run Self-Consistency (no embeddings), score, save, ablation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87d6b4ba-fe64-4514-96e0-4c6cff19819f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================================================================\n",
      "[SELF-CONSISTENCY] k=3 | model=openai/gpt-oss-120b\n",
      "========================================================================================\n",
      "\n",
      "========================================================================================\n",
      "[SELF-CONSISTENCY / CoT PROMPT]\n",
      "\n",
      "[SYSTEM]\n",
      "You are a concise, evidence-focused medical assistant. Reason step-by-step using brief bullet points, then provide a final answer. If unsure, say you don't know.\n",
      "\n",
      "[USER]\n",
      "Question: Do you have information about X-Rays\n",
      "\n",
      "Follow this exact format:\n",
      "Reasoning:\n",
      "- bullet 1\n",
      "- bullet 2\n",
      "- bullet 3\n",
      "Final Answer: <one concise sentence>\n",
      "\n",
      "Be brief and avoid speculation.\n",
      "========================================================================================\n",
      "\n",
      "========================================================================================\n",
      "[SELF-CONSISTENCY / CoT PROMPT]\n",
      "\n",
      "[SYSTEM]\n",
      "You are a concise, evidence-focused medical assistant. Reason step-by-step using brief bullet points, then provide a final answer. If unsure, say you don't know.\n",
      "\n",
      "[USER]\n",
      "Question: What are the symptoms of Alpha-ketoglutarate dehydrogenase deficiency ?\n",
      "\n",
      "Follow this exact format:\n",
      "Reasoning:\n",
      "- bullet 1\n",
      "- bullet 2\n",
      "- bullet 3\n",
      "Final Answer: <one concise sentence>\n",
      "\n",
      "Be brief and avoid speculation.\n",
      "========================================================================================\n",
      "\n",
      "========================================================================================\n",
      "[SELF-CONSISTENCY / CoT PROMPT]\n",
      "\n",
      "[SYSTEM]\n",
      "You are a concise, evidence-focused medical assistant. Reason step-by-step using brief bullet points, then provide a final answer. If unsure, say you don't know.\n",
      "\n",
      "[USER]\n",
      "Question: What are the treatments for GLUT1 deficiency syndrome ?\n",
      "\n",
      "Follow this exact format:\n",
      "Reasoning:\n",
      "- bullet 1\n",
      "- bullet 2\n",
      "- bullet 3\n",
      "Final Answer: <one concise sentence>\n",
      "\n",
      "Be brief and avoid speculation.\n",
      "========================================================================================\n",
      "\n",
      "[Cost/Run] k=3 | gen_calls=9 | judge_calls≈48 | elapsed=50.9s\n",
      "[Saved] traces → ./sc_k3_samples.csv\n",
      "[Saved] scores → ./sc_k3_scores.csv\n",
      "\n",
      "========================================================================================\n",
      "[SELF-CONSISTENCY] k=5 | model=openai/gpt-oss-120b\n",
      "========================================================================================\n",
      "\n",
      "========================================================================================\n",
      "[SELF-CONSISTENCY / CoT PROMPT]\n",
      "\n",
      "[SYSTEM]\n",
      "You are a concise, evidence-focused medical assistant. Reason step-by-step using brief bullet points, then provide a final answer. If unsure, say you don't know.\n",
      "\n",
      "[USER]\n",
      "Question: Do you have information about X-Rays\n",
      "\n",
      "Follow this exact format:\n",
      "Reasoning:\n",
      "- bullet 1\n",
      "- bullet 2\n",
      "- bullet 3\n",
      "Final Answer: <one concise sentence>\n",
      "\n",
      "Be brief and avoid speculation.\n",
      "========================================================================================\n",
      "\n",
      "========================================================================================\n",
      "[SELF-CONSISTENCY / CoT PROMPT]\n",
      "\n",
      "[SYSTEM]\n",
      "You are a concise, evidence-focused medical assistant. Reason step-by-step using brief bullet points, then provide a final answer. If unsure, say you don't know.\n",
      "\n",
      "[USER]\n",
      "Question: What are the symptoms of Alpha-ketoglutarate dehydrogenase deficiency ?\n",
      "\n",
      "Follow this exact format:\n",
      "Reasoning:\n",
      "- bullet 1\n",
      "- bullet 2\n",
      "- bullet 3\n",
      "Final Answer: <one concise sentence>\n",
      "\n",
      "Be brief and avoid speculation.\n",
      "========================================================================================\n",
      "\n",
      "========================================================================================\n",
      "[SELF-CONSISTENCY / CoT PROMPT]\n",
      "\n",
      "[SYSTEM]\n",
      "You are a concise, evidence-focused medical assistant. Reason step-by-step using brief bullet points, then provide a final answer. If unsure, say you don't know.\n",
      "\n",
      "[USER]\n",
      "Question: What are the treatments for GLUT1 deficiency syndrome ?\n",
      "\n",
      "Follow this exact format:\n",
      "Reasoning:\n",
      "- bullet 1\n",
      "- bullet 2\n",
      "- bullet 3\n",
      "Final Answer: <one concise sentence>\n",
      "\n",
      "Be brief and avoid speculation.\n",
      "========================================================================================\n",
      "\n",
      "[Cost/Run] k=5 | gen_calls=15 | judge_calls≈132 | elapsed=289.1s\n",
      "[Saved] traces → ./sc_k5_samples.csv\n",
      "[Saved] scores → ./sc_k5_scores.csv\n",
      "\n",
      "=== SELF-CONSISTENCY ABLATION SUMMARY (higher is better) ===\n",
      " k  faithfulness  hallucination_rate  answer_relevance  answer_correctness\n",
      " 3         0.650               0.350             0.908               0.920\n",
      " 5         0.914               0.086             0.842               0.847\n",
      "Saved: ./self_consistency_ablation_summary.csv\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "from pathlib import Path\n",
    "\n",
    "ALL_SUMMARIES = []\n",
    "for K in K_LIST:\n",
    "    print(\"\\n\" + \"=\"*88)\n",
    "    print(f\"[SELF-CONSISTENCY] k={K} | model={GEN_MODEL}\")\n",
    "    print(\"=\"*88)\n",
    "\n",
    "    GEN_CALLS = 0\n",
    "    JUDGE_CALLS = 0\n",
    "    t0 = time()\n",
    "\n",
    "    rows = []\n",
    "    for _, r in eval_df.iterrows():\n",
    "        q    = normalize_text(str(r[\"question\"]))\n",
    "        gold = normalize_text(str(r[\"gold\"]))\n",
    "\n",
    "        # 1) generate k CoT samples\n",
    "        df_k = generate_cot_samples(q, k=K)\n",
    "        GEN_CALLS += K\n",
    "        candidates = df_k[\"cot_final\"].astype(str).tolist()\n",
    "\n",
    "        # 2) majority vote (no embeddings)\n",
    "        idx, diag = majority_consensus(candidates, min_votes=2)\n",
    "\n",
    "        # 3) tie-break via LLM entailment medoid (still prompting-based)\n",
    "        if idx is None:\n",
    "            idx = tie_break_by_entailment(candidates)\n",
    "            diag[\"method\"] = \"llm_entailment_tiebreak\"\n",
    "            diag[\"chosen_index\"] = int(idx)\n",
    "\n",
    "        best_answer = candidates[idx]\n",
    "\n",
    "        # 4) score with LLM judge\n",
    "        probs, soft_faith, hard = faithfulness_metrics(best_answer, gold, hard_thresh_list=(0.5, 0.8))\n",
    "        relev = answer_relevance(q, best_answer)\n",
    "        corr  = answer_correctness_llm(gold, best_answer)\n",
    "\n",
    "        rows.append({\n",
    "            \"k\": K,\n",
    "            \"question\": q,\n",
    "            \"gold\": gold,\n",
    "            \"consensus_answer\": best_answer,\n",
    "            \"faithfulness\": round(soft_faith, 3),\n",
    "            \"hallucination_rate\": round(1.0 - soft_faith, 3),\n",
    "            \"faithfulness_hard@0.5\": round(hard[0.5], 3),\n",
    "            \"faithfulness_hard@0.8\": round(hard[0.8], 3),\n",
    "            \"answer_relevance\": round(relev, 3),\n",
    "            \"answer_correctness\": round(corr, 3),\n",
    "            \"faith_band\": band(soft_faith),\n",
    "            \"relevance_band\": band(relev),\n",
    "            \"correctness_band\": band(corr),\n",
    "            # diagnostics\n",
    "            \"sc_method\": diag.get(\"method\"),\n",
    "            \"sc_votes_or_idx\": diag.get(\"votes\", diag.get(\"chosen_index\")),\n",
    "        })\n",
    "\n",
    "        # Save raw samples per question to a trace CSV (append-friendly)\n",
    "        trace_csv = f\"{RESULTS_DIR}/sc_k{K}_samples.csv\"\n",
    "        df_k_tmp = df_k.copy()\n",
    "        df_k_tmp.insert(0, \"question\", q)\n",
    "        mode = \"a\" if Path(trace_csv).exists() else \"w\"\n",
    "        header = not Path(trace_csv).exists()\n",
    "        df_k_tmp.to_csv(trace_csv, index=False, mode=mode, header=header)\n",
    "\n",
    "    # Save per-k scored results\n",
    "    run_df = pd.DataFrame(rows)\n",
    "    out_csv = f\"{RESULTS_DIR}/sc_k{K}_scores.csv\"\n",
    "    run_df.to_csv(out_csv, index=False)\n",
    "\n",
    "    elapsed = time() - t0\n",
    "    print(f\"\\n[Cost/Run] k={K} | gen_calls={GEN_CALLS} | judge_calls≈{JUDGE_CALLS} | elapsed={elapsed:.1f}s\")\n",
    "    print(f\"[Saved] traces → {RESULTS_DIR}/sc_k{K}_samples.csv\")\n",
    "    print(f\"[Saved] scores → {out_csv}\")\n",
    "\n",
    "    # 5/7) Aggregate per k\n",
    "    summary = run_df[[\"faithfulness\",\"hallucination_rate\",\"answer_relevance\",\"answer_correctness\"]].mean().round(3)\n",
    "    summary = summary.to_frame().T\n",
    "    summary.insert(0, \"k\", K)\n",
    "    ALL_SUMMARIES.append(summary)\n",
    "\n",
    "# Final ablation table\n",
    "ablate = pd.concat(ALL_SUMMARIES, ignore_index=True)\n",
    "ablate_csv = f\"{RESULTS_DIR}/self_consistency_ablation_summary.csv\"\n",
    "ablate.to_csv(ablate_csv, index=False)\n",
    "print(\"\\n=== SELF-CONSISTENCY ABLATION SUMMARY (higher is better) ===\")\n",
    "print(ablate.to_string(index=False))\n",
    "print(\"Saved:\", ablate_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd303e94-4f4c-4e93-98df-be3eed141d49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca008ad7-5c4f-4538-b34e-ac578b666e9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
