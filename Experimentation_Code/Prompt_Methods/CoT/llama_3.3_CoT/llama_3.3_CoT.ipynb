{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "add734ab-0ae9-44d6-b83b-ff042e1fb481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\anish\\anaconda3\\lib\\site-packages (23.3.1)\n",
      "Collecting pip\n",
      "  Using cached pip-25.2-py3-none-any.whl.metadata (4.7 kB)\n",
      "Using cached pip-25.2-py3-none-any.whl (1.8 MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: To modify pip, please run the following command:\n",
      "C:\\Users\\anish\\anaconda3\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: groq in c:\\users\\anish\\anaconda3\\lib\\site-packages (0.31.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\anish\\anaconda3\\lib\\site-packages (2.1.4)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from groq) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from groq) (1.8.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from groq) (0.27.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from groq) (2.8.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\anish\\anaconda3\\lib\\site-packages (from groq) (1.3.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from groq) (4.12.2)\n",
      "Requirement already satisfied: numpy<2,>=1.23.2 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from anyio<5,>=3.5.0->groq) (3.4)\n",
      "Requirement already satisfied: certifi in c:\\users\\anish\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->groq) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\anish\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->groq) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (2.20.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install groq pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d08e8955-2eb8-4d1c-aaa8-1db4b91cf25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Paths / Model config ===\n",
    "EVAL_SELECTION_CSV = \"medquad_selected_questions.csv\"  # same questions file you use elsewhere\n",
    "ANSWER_FIELD       = \"answer\"                                       # if your CSV uses 'gold', we auto-detect below\n",
    "N_EVAL             = 3                                              # set to an int, or None to use all rows\n",
    "\n",
    "# LLMs\n",
    "GROQ_API_KEY = \"\"      # <-- put your real key here (hard-coded as requested)\n",
    "GEN_MODEL    = \"llama-3.3-70b-versatile\"       # generator; if unavailable for your tier, use \"llama-3.1-8b-instant\"\n",
    "JUDGE_MODEL  = \"llama-3.1-8b-instant\"          # LLM-as-judge (fast/cheap)\n",
    "\n",
    "# Generation knobs (keep fixed across experiments)\n",
    "TEMPERATURE = 0.0\n",
    "MAX_TOKENS  = 512\n",
    "TOP_P       = 1.0\n",
    "\n",
    "# Prompt visibility\n",
    "PRINT_COT_PROMPTS = True   # True = print the exact CoT prompt sent to the LLM\n",
    "\n",
    "# Outputs\n",
    "RESULTS_DIR = \".\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb138bd-4ecf-48d0-a0e0-6cb33ad22dca",
   "metadata": {},
   "source": [
    "### Load questions (same CSV you use for other runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ed12ff4-88eb-4c64-8dad-ce113ea4fe4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CoT evaluation questions: 3\n",
      "              Do you have information about X-Rays\n",
      "What are the symptoms of Alpha-ketoglutarate de...\n",
      "What are the treatments for GLUT1 deficiency sy...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sel = pd.read_csv(EVAL_SELECTION_CSV)\n",
    "\n",
    "# figure out which column holds the gold/reference text\n",
    "gold_col = \"gold\" if \"gold\" in sel.columns else (ANSWER_FIELD if ANSWER_FIELD in sel.columns else None)\n",
    "assert gold_col is not None, f\"Selection file must contain either 'gold' or '{ANSWER_FIELD}'\"\n",
    "\n",
    "eval_df = sel[[\"question\", gold_col]].copy()\n",
    "eval_df.columns = [\"question\", \"gold\"]   # internal rename for convenience\n",
    "\n",
    "if isinstance(N_EVAL, int):\n",
    "    eval_df = eval_df.head(N_EVAL)\n",
    "\n",
    "print(\"CoT evaluation questions:\", len(eval_df))\n",
    "print(eval_df[\"question\"].to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0f18f8-b4ac-43e9-b303-244df2b5c666",
   "metadata": {},
   "source": [
    "### Groq client + chat helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bd5aec0-ddd9-418e-9e80-455d45da14de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "from groq import Groq\n",
    "\n",
    "client = Groq(api_key=GROQ_API_KEY)\n",
    "\n",
    "def chat_messages(model: str, messages: List[Dict], temperature: float = 0.0, max_tokens: int = 256, top_p: float = 1.0) -> str:\n",
    "    \"\"\"\n",
    "    Thin wrapper around Groq chat completions API.\n",
    "    \"\"\"\n",
    "    r = client.chat.completions.create(\n",
    "        model=model, temperature=temperature, max_tokens=max_tokens, top_p=top_p,\n",
    "        messages=messages\n",
    "    )\n",
    "    return r.choices[0].message.content.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c18563-567b-4e1b-9422-fa5d10239806",
   "metadata": {},
   "source": [
    "### Static CoT prompt builder (prints the exact prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80c3e0ef-b794-4d36-b3ac-ce4884b7b23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cot_messages(question: str, print_prompt: bool = False) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Static CoT template: only the 'question' text changes.\n",
    "    Reuse this exact template with any LLM to compare models fairly.\n",
    "    \"\"\"\n",
    "    system_msg = (\n",
    "        \"You are a concise, evidence-focused medical assistant. \"\n",
    "        \"Reason step-by-step, then provide a final answer. \"\n",
    "        \"If unsure, say you don't know.\"\n",
    "    )\n",
    "    user_msg = (\n",
    "        \"Question: \" + question + \"\\n\\n\"\n",
    "        \"Follow this exact format:\\n\"\n",
    "        \"Reasoning:\\n\"\n",
    "        \"- bullet 1\\n- bullet 2\\n- bullet 3\\n\"\n",
    "        \"Final Answer: <one concise sentence>\\n\\n\"\n",
    "        \"Be brief and avoid speculation.\"\n",
    "    )\n",
    "    messages = [{\"role\":\"system\",\"content\":system_msg},\n",
    "                {\"role\":\"user\",\"content\":user_msg}]\n",
    "    if print_prompt:\n",
    "        print(\"\\n\" + \"=\"*88)\n",
    "        print(\"[CoT PROMPT]\")\n",
    "        print(\"\\n[SYSTEM]\\n\" + system_msg)\n",
    "        print(\"\\n[USER]\\n\" + user_msg)\n",
    "        print(\"=\"*88)\n",
    "    return messages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65225b7-96c3-4c6a-b891-399d1ace522d",
   "metadata": {},
   "source": [
    "### Normalizer + CoT parser (strip reasoning before scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3cd8ba98-d5cf-4c49-ac31-7094c0dda42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, unicodedata\n",
    "\n",
    "def normalize_text(t: str) -> str:\n",
    "    t = (t or \"\").strip()\n",
    "    t = unicodedata.normalize(\"NFKC\", t)\n",
    "    t = re.sub(r\"\\s+\", \" \", t)\n",
    "    return t\n",
    "\n",
    "def parse_cot(text: str):\n",
    "    \"\"\"\n",
    "    Extract 'Reasoning' and 'Final Answer' from model output.\n",
    "    Returns (reasoning_text, final_answer_text, cleaned_full_text).\n",
    "    Scoring uses only the Final Answer (to keep it fair).\n",
    "    \"\"\"\n",
    "    raw = normalize_text(text)\n",
    "    m = re.search(r\"(?:Final\\s*Answer\\s*:\\s*)(.*)$\", raw, flags=re.IGNORECASE|re.DOTALL)\n",
    "    if m:\n",
    "        final = m.group(1).strip()\n",
    "        reasoning = raw[:m.start()].strip()\n",
    "        return reasoning, final, raw\n",
    "    # Fallback: last sentence as final\n",
    "    parts = re.split(r'(?<=[.!?])\\s+', raw)\n",
    "    final = parts[-1].strip() if parts else raw\n",
    "    reasoning = raw[:max(0, raw.rfind(final))].strip()\n",
    "    return reasoning, final, raw\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437f4c05-19f9-4508-8d1d-be72edb3faeb",
   "metadata": {},
   "source": [
    "### Run CoT (prints prompt, saves traces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bd317b8-c438-477c-bd31-ecf5128eeaec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================================================================\n",
      "[CoT PROMPT]\n",
      "\n",
      "[SYSTEM]\n",
      "You are a concise, evidence-focused medical assistant. Reason step-by-step, then provide a final answer. If unsure, say you don't know.\n",
      "\n",
      "[USER]\n",
      "Question: Do you have information about X-Rays\n",
      "\n",
      "Follow this exact format:\n",
      "Reasoning:\n",
      "- bullet 1\n",
      "- bullet 2\n",
      "- bullet 3\n",
      "Final Answer: <one concise sentence>\n",
      "\n",
      "Be brief and avoid speculation.\n",
      "========================================================================================\n",
      "\n",
      "========================================================================================\n",
      "[CoT PROMPT]\n",
      "\n",
      "[SYSTEM]\n",
      "You are a concise, evidence-focused medical assistant. Reason step-by-step, then provide a final answer. If unsure, say you don't know.\n",
      "\n",
      "[USER]\n",
      "Question: What are the symptoms of Alpha-ketoglutarate dehydrogenase deficiency ?\n",
      "\n",
      "Follow this exact format:\n",
      "Reasoning:\n",
      "- bullet 1\n",
      "- bullet 2\n",
      "- bullet 3\n",
      "Final Answer: <one concise sentence>\n",
      "\n",
      "Be brief and avoid speculation.\n",
      "========================================================================================\n",
      "\n",
      "========================================================================================\n",
      "[CoT PROMPT]\n",
      "\n",
      "[SYSTEM]\n",
      "You are a concise, evidence-focused medical assistant. Reason step-by-step, then provide a final answer. If unsure, say you don't know.\n",
      "\n",
      "[USER]\n",
      "Question: What are the treatments for GLUT1 deficiency syndrome ?\n",
      "\n",
      "Follow this exact format:\n",
      "Reasoning:\n",
      "- bullet 1\n",
      "- bullet 2\n",
      "- bullet 3\n",
      "Final Answer: <one concise sentence>\n",
      "\n",
      "Be brief and avoid speculation.\n",
      "========================================================================================\n",
      "[CoT] Saved CoT traces to: ./medquad_cot_traces.csv  (rows=3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>gold</th>\n",
       "      <th>strategy</th>\n",
       "      <th>cot_reasoning</th>\n",
       "      <th>cot_final</th>\n",
       "      <th>answer</th>\n",
       "      <th>raw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Do you have information about X-Rays</td>\n",
       "      <td>Summary : X-rays are a type of radiation calle...</td>\n",
       "      <td>cot</td>\n",
       "      <td>Reasoning: - X-Rays are a form of electromagne...</td>\n",
       "      <td>X-Rays are a medical imaging tool used to visu...</td>\n",
       "      <td>X-Rays are a medical imaging tool used to visu...</td>\n",
       "      <td>Reasoning: - X-Rays are a form of electromagne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are the symptoms of Alpha-ketoglutarate d...</td>\n",
       "      <td>What are the signs and symptoms of Alpha-ketog...</td>\n",
       "      <td>cot</td>\n",
       "      <td>Reasoning: - Alpha-ketoglutarate dehydrogenase...</td>\n",
       "      <td>Alpha-ketoglutarate dehydrogenase deficiency s...</td>\n",
       "      <td>Alpha-ketoglutarate dehydrogenase deficiency s...</td>\n",
       "      <td>Reasoning: - Alpha-ketoglutarate dehydrogenase...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0               Do you have information about X-Rays   \n",
       "1  What are the symptoms of Alpha-ketoglutarate d...   \n",
       "\n",
       "                                                gold strategy  \\\n",
       "0  Summary : X-rays are a type of radiation calle...      cot   \n",
       "1  What are the signs and symptoms of Alpha-ketog...      cot   \n",
       "\n",
       "                                       cot_reasoning  \\\n",
       "0  Reasoning: - X-Rays are a form of electromagne...   \n",
       "1  Reasoning: - Alpha-ketoglutarate dehydrogenase...   \n",
       "\n",
       "                                           cot_final  \\\n",
       "0  X-Rays are a medical imaging tool used to visu...   \n",
       "1  Alpha-ketoglutarate dehydrogenase deficiency s...   \n",
       "\n",
       "                                              answer  \\\n",
       "0  X-Rays are a medical imaging tool used to visu...   \n",
       "1  Alpha-ketoglutarate dehydrogenase deficiency s...   \n",
       "\n",
       "                                                 raw  \n",
       "0  Reasoning: - X-Rays are a form of electromagne...  \n",
       "1  Reasoning: - Alpha-ketoglutarate dehydrogenase...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def run_cot(print_prompts: bool = False) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for _, r in eval_df.iterrows():\n",
    "        q   = normalize_text(str(r[\"question\"]))\n",
    "        gold= normalize_text(str(r[\"gold\"]))\n",
    "\n",
    "        msgs = build_cot_messages(q, print_prompt=print_prompts)\n",
    "        out  = chat_messages(GEN_MODEL, msgs, temperature=TEMPERATURE, max_tokens=MAX_TOKENS, top_p=TOP_P)\n",
    "\n",
    "        reasoning, final, raw = parse_cot(out)\n",
    "\n",
    "        rows.append({\n",
    "            \"question\": q,\n",
    "            \"gold\": gold,\n",
    "            \"strategy\": \"cot\",\n",
    "            \"cot_reasoning\": reasoning,\n",
    "            \"cot_final\": final,\n",
    "            \"answer\": final,    # scoring uses only the final answer\n",
    "            \"raw\": raw\n",
    "        })\n",
    "\n",
    "    df_cot = pd.DataFrame(rows)\n",
    "    out_csv = f\"{RESULTS_DIR}/medquad_cot_traces.csv\"\n",
    "    df_cot.to_csv(out_csv, index=False)\n",
    "    print(f\"[CoT] Saved CoT traces to: {out_csv}  (rows={len(df_cot)})\")\n",
    "    return df_cot\n",
    "\n",
    "cot_df = run_cot(print_prompts=PRINT_COT_PROMPTS)\n",
    "cot_df.head(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338d1fb4-3167-4907-a3cd-34eeb0f3fcf8",
   "metadata": {},
   "source": [
    "### LLM-as-judge metrics: Faithfulness, Hallucination, Answer Relevance, Answer Correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28688b75-85a9-4e00-93f5-148059bd0700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved CoT per-item scores to: ./medquad_cot_scores.csv\n",
      "\n",
      "=== CoT Summary (averages) ===\n",
      "faithfulness          0.333\n",
      "hallucination_rate    0.667\n",
      "answer_relevance      0.733\n",
      "answer_correctness    0.691\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# --- FIXED SCORING CELL (drop-in replacement) ---\n",
    "import re\n",
    "import pandas as pd  # â† import at top-level so it's always available\n",
    "\n",
    "JUDGE_TEMPERATURE = 0.0\n",
    "JUDGE_MAX_TOKENS  = 64\n",
    "JUDGE_TOP_P       = 1.0\n",
    "\n",
    "def _extract_float(txt: str) -> float:\n",
    "    m = re.search(r\"\\d*\\.?\\d+(?:[eE][-+]?\\d+)?\", txt or \"\")\n",
    "    try: x = float(m.group(0)) if m else 0.0\n",
    "    except: x = 0.0\n",
    "    return max(0.0, min(1.0, x))\n",
    "\n",
    "def chat_judge(system_prompt: str, user_prompt: str, model: str = JUDGE_MODEL) -> str:\n",
    "    r = client.chat.completions.create(\n",
    "        model=model, temperature=JUDGE_TEMPERATURE, max_tokens=JUDGE_MAX_TOKENS, top_p=JUDGE_TOP_P,\n",
    "        messages=[{\"role\":\"system\",\"content\":system_prompt},\n",
    "                  {\"role\":\"user\",\"content\":user_prompt}]\n",
    "    )\n",
    "    return r.choices[0].message.content.strip()\n",
    "\n",
    "def entail_prob(premise: str, claim: str) -> float:\n",
    "    sys = (\"You are an evaluator. Given a PREMISE (evidence) and a CLAIM (one sentence), \"\n",
    "           \"return ONLY a number in [0,1] = probability that PREMISE ENTAILS CLAIM.\")\n",
    "    usr = f\"PREMISE:\\n{premise}\\n\\nCLAIM:\\n{claim}\\n\\nOutput only a number in [0,1].\"\n",
    "    return _extract_float(chat_judge(sys, usr))\n",
    "\n",
    "def split_sents(t: str):\n",
    "    t = (t or \"\").strip()\n",
    "    return [s.strip() for s in re.split(r'(?<=[.!?])\\s+', t) if s.strip()]\n",
    "\n",
    "def faithfulness_verbose(answer: str, gold_reference: str, thresh: float = 0.5):\n",
    "    \"\"\"\n",
    "    Treat the gold answer as the evidence. Faithfulness = fraction of answer sentences\n",
    "    that are entailed by the gold reference at probability >= thresh.\n",
    "    \"\"\"\n",
    "    sents = split_sents(answer)\n",
    "    if not sents:\n",
    "        return 0.0, 1.0, pd.DataFrame(columns=[\"sentence\",\"entail_prob\",\"supported\"])\n",
    "    rows, ok = [], 0\n",
    "    for s in sents:\n",
    "        p = entail_prob(gold_reference, s)\n",
    "        sup = p >= thresh\n",
    "        ok += int(sup)\n",
    "        rows.append({\"sentence\": s, \"entail_prob\": round(p,3), \"supported\": sup})\n",
    "    f = ok/len(sents)\n",
    "    return f, 1.0-f, pd.DataFrame(rows)\n",
    "\n",
    "def answer_correctness_llm(gold_answer: str, model_answer: str) -> float:\n",
    "    \"\"\"Bidirectional entailment: 0.5*(E(gold->answer)+E(answer->gold)).\"\"\"\n",
    "    e1 = entail_prob(gold_answer, model_answer)\n",
    "    e2 = entail_prob(model_answer, gold_answer)\n",
    "    return 0.5*(e1+e2)\n",
    "\n",
    "def answer_relevance(q: str, a: str) -> float:\n",
    "    sys = (\n",
    "        \"You are an evaluator. Rate how well the ANSWER addresses the QUESTION.\\n\"\n",
    "        \"- 1.0 = Directly answers, accurate and focused.\\n\"\n",
    "        \"- 0.7 = Mostly answers with minor gaps/irrelevance.\\n\"\n",
    "        \"- 0.4 = Partial answer; noticeable gaps or off-topic parts.\\n\"\n",
    "        \"- 0.0 = Does not answer or off-topic.\\n\"\n",
    "        \"Return ONLY a number in [0,1].\"\n",
    "    )\n",
    "    usr = f\"QUESTION:\\n{q}\\n\\nANSWER:\\n{a}\\n\\nScore:\"\n",
    "    return _extract_float(chat_judge(sys, usr))\n",
    "\n",
    "def band(x: float) -> str:\n",
    "    return \"Excellent\" if x>=0.90 else \"Good\" if x>=0.75 else \"Borderline\" if x>=0.60 else \"Poor\"\n",
    "\n",
    "def score_cot(df_answers: pd.DataFrame, faith_thresh: float = 0.5) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for _, r in df_answers.iterrows():\n",
    "        q, a, g = r[\"question\"], r[\"answer\"], r[\"gold\"]\n",
    "        faith, halluc, _df = faithfulness_verbose(a, g, thresh=faith_thresh)\n",
    "        relev = answer_relevance(q, a)\n",
    "        corr  = answer_correctness_llm(g, a)\n",
    "        rows.append({\n",
    "            \"question\": q,\n",
    "            \"strategy\": \"cot\",\n",
    "            \"faithfulness\": round(faith, 3),\n",
    "            \"hallucination_rate\": round(halluc, 3),\n",
    "            \"answer_relevance\": round(relev, 3),\n",
    "            \"answer_correctness\": round(corr, 3),\n",
    "            \"faith_band\": band(faith),\n",
    "            \"relevance_band\": band(relev),\n",
    "            \"correctness_band\": band(corr),\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# run scoring\n",
    "cot_scored = score_cot(cot_df, faith_thresh=0.5)\n",
    "cot_scored.to_csv(f\"{RESULTS_DIR}/medquad_cot_scores.csv\", index=False)\n",
    "print(\"Saved CoT per-item scores to:\", f\"{RESULTS_DIR}/medquad_cot_scores.csv\")\n",
    "\n",
    "print(\"\\n=== CoT Summary (averages) ===\")\n",
    "print(cot_scored[[\"faithfulness\",\"hallucination_rate\",\"answer_relevance\",\"answer_correctness\"]].mean().round(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fda2893-3f42-4623-b107-d21c03afa0fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa07ed99-c138-4a83-b43e-018e076189c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
