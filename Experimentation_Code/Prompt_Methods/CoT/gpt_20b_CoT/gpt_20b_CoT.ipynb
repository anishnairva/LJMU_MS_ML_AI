{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "add734ab-0ae9-44d6-b83b-ff042e1fb481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\anish\\anaconda3\\lib\\site-packages (23.3.1)\n",
      "Collecting pip\n",
      "  Using cached pip-25.2-py3-none-any.whl.metadata (4.7 kB)\n",
      "Using cached pip-25.2-py3-none-any.whl (1.8 MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: To modify pip, please run the following command:\n",
      "C:\\Users\\anish\\anaconda3\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: groq in c:\\users\\anish\\anaconda3\\lib\\site-packages (0.31.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\anish\\anaconda3\\lib\\site-packages (2.1.4)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from groq) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from groq) (1.8.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from groq) (0.27.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from groq) (2.8.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\anish\\anaconda3\\lib\\site-packages (from groq) (1.3.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from groq) (4.12.2)\n",
      "Requirement already satisfied: numpy<2,>=1.23.2 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from anyio<5,>=3.5.0->groq) (3.4)\n",
      "Requirement already satisfied: certifi in c:\\users\\anish\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->groq) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\anish\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->groq) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (2.20.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install groq pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d08e8955-2eb8-4d1c-aaa8-1db4b91cf25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Paths / Model config ===\n",
    "EVAL_SELECTION_CSV = \"medquad_selected_questions.csv\"  # same questions file you use elsewhere\n",
    "ANSWER_FIELD       = \"answer\"                                       # if your CSV uses 'gold', we auto-detect below\n",
    "N_EVAL             = 3                                              # set to an int, or None to use all rows\n",
    "\n",
    "# LLMs\n",
    "GROQ_API_KEY = \"\"      # <-- put your real key here\n",
    "GEN_MODEL    = \"openai/gpt-oss-20b\"       # generator; if unavailable for your tier, use \"llama-3.1-8b-instant\"\n",
    "JUDGE_MODEL  = \"llama-3.1-8b-instant\"          # LLM-as-judge (fast/cheap)\n",
    "\n",
    "# Generation knobs (keep fixed across experiments)\n",
    "TEMPERATURE = 0.0\n",
    "MAX_TOKENS  = 512\n",
    "TOP_P       = 1.0\n",
    "\n",
    "# Prompt visibility\n",
    "PRINT_COT_PROMPTS = True   # True = print the exact CoT prompt sent to the LLM\n",
    "\n",
    "# Outputs\n",
    "RESULTS_DIR = \".\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb138bd-4ecf-48d0-a0e0-6cb33ad22dca",
   "metadata": {},
   "source": [
    "### Load questions (same CSV you use for other runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ed12ff4-88eb-4c64-8dad-ce113ea4fe4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CoT evaluation questions: 3\n",
      "              Do you have information about X-Rays\n",
      "What are the symptoms of Alpha-ketoglutarate de...\n",
      "What are the treatments for GLUT1 deficiency sy...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sel = pd.read_csv(EVAL_SELECTION_CSV)\n",
    "\n",
    "# figure out which column holds the gold/reference text\n",
    "gold_col = \"gold\" if \"gold\" in sel.columns else (ANSWER_FIELD if ANSWER_FIELD in sel.columns else None)\n",
    "assert gold_col is not None, f\"Selection file must contain either 'gold' or '{ANSWER_FIELD}'\"\n",
    "\n",
    "eval_df = sel[[\"question\", gold_col]].copy()\n",
    "eval_df.columns = [\"question\", \"gold\"]   # internal rename for convenience\n",
    "\n",
    "if isinstance(N_EVAL, int):\n",
    "    eval_df = eval_df.head(N_EVAL)\n",
    "\n",
    "print(\"CoT evaluation questions:\", len(eval_df))\n",
    "print(eval_df[\"question\"].to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0f18f8-b4ac-43e9-b303-244df2b5c666",
   "metadata": {},
   "source": [
    "### Groq client + chat helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bd5aec0-ddd9-418e-9e80-455d45da14de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "from groq import Groq\n",
    "\n",
    "client = Groq(api_key=GROQ_API_KEY)\n",
    "\n",
    "def chat_messages(model: str, messages: List[Dict], temperature: float = 0.0, max_tokens: int = 256, top_p: float = 1.0) -> str:\n",
    "    \"\"\"\n",
    "    Thin wrapper around Groq chat completions API.\n",
    "    \"\"\"\n",
    "    r = client.chat.completions.create(\n",
    "        model=model, temperature=temperature, max_tokens=max_tokens, top_p=top_p,\n",
    "        messages=messages\n",
    "    )\n",
    "    return r.choices[0].message.content.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c18563-567b-4e1b-9422-fa5d10239806",
   "metadata": {},
   "source": [
    "### Static CoT prompt builder (prints the exact prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80c3e0ef-b794-4d36-b3ac-ce4884b7b23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cot_messages(question: str, print_prompt: bool = False) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Static CoT template: only the 'question' text changes.\n",
    "    Reuse this exact template with any LLM to compare models fairly.\n",
    "    \"\"\"\n",
    "    system_msg = (\n",
    "        \"You are a concise, evidence-focused medical assistant. \"\n",
    "        \"Reason step-by-step, then provide a final answer. \"\n",
    "        \"If unsure, say you don't know.\"\n",
    "    )\n",
    "    user_msg = (\n",
    "        \"Question: \" + question + \"\\n\\n\"\n",
    "        \"Follow this exact format:\\n\"\n",
    "        \"Reasoning:\\n\"\n",
    "        \"- bullet 1\\n- bullet 2\\n- bullet 3\\n\"\n",
    "        \"Final Answer: <one concise sentence>\\n\\n\"\n",
    "        \"Be brief and avoid speculation.\"\n",
    "    )\n",
    "    messages = [{\"role\":\"system\",\"content\":system_msg},\n",
    "                {\"role\":\"user\",\"content\":user_msg}]\n",
    "    if print_prompt:\n",
    "        print(\"\\n\" + \"=\"*88)\n",
    "        print(\"[CoT PROMPT]\")\n",
    "        print(\"\\n[SYSTEM]\\n\" + system_msg)\n",
    "        print(\"\\n[USER]\\n\" + user_msg)\n",
    "        print(\"=\"*88)\n",
    "    return messages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65225b7-96c3-4c6a-b891-399d1ace522d",
   "metadata": {},
   "source": [
    "### Normalizer + CoT parser (strip reasoning before scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3cd8ba98-d5cf-4c49-ac31-7094c0dda42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, unicodedata\n",
    "\n",
    "def normalize_text(t: str) -> str:\n",
    "    t = (t or \"\").strip()\n",
    "    t = unicodedata.normalize(\"NFKC\", t)\n",
    "    t = re.sub(r\"\\s+\", \" \", t)\n",
    "    return t\n",
    "\n",
    "def parse_cot(text: str):\n",
    "    \"\"\"\n",
    "    Extract 'Reasoning' and 'Final Answer' from model output.\n",
    "    Returns (reasoning_text, final_answer_text, cleaned_full_text).\n",
    "    Scoring uses only the Final Answer (to keep it fair).\n",
    "    \"\"\"\n",
    "    raw = normalize_text(text)\n",
    "    m = re.search(r\"(?:Final\\s*Answer\\s*:\\s*)(.*)$\", raw, flags=re.IGNORECASE|re.DOTALL)\n",
    "    if m:\n",
    "        final = m.group(1).strip()\n",
    "        reasoning = raw[:m.start()].strip()\n",
    "        return reasoning, final, raw\n",
    "    # Fallback: last sentence as final\n",
    "    parts = re.split(r'(?<=[.!?])\\s+', raw)\n",
    "    final = parts[-1].strip() if parts else raw\n",
    "    reasoning = raw[:max(0, raw.rfind(final))].strip()\n",
    "    return reasoning, final, raw\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437f4c05-19f9-4508-8d1d-be72edb3faeb",
   "metadata": {},
   "source": [
    "### Run CoT (prints prompt, saves traces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bd317b8-c438-477c-bd31-ecf5128eeaec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================================================================\n",
      "[CoT PROMPT]\n",
      "\n",
      "[SYSTEM]\n",
      "You are a concise, evidence-focused medical assistant. Reason step-by-step, then provide a final answer. If unsure, say you don't know.\n",
      "\n",
      "[USER]\n",
      "Question: Do you have information about X-Rays\n",
      "\n",
      "Follow this exact format:\n",
      "Reasoning:\n",
      "- bullet 1\n",
      "- bullet 2\n",
      "- bullet 3\n",
      "Final Answer: <one concise sentence>\n",
      "\n",
      "Be brief and avoid speculation.\n",
      "========================================================================================\n",
      "\n",
      "========================================================================================\n",
      "[CoT PROMPT]\n",
      "\n",
      "[SYSTEM]\n",
      "You are a concise, evidence-focused medical assistant. Reason step-by-step, then provide a final answer. If unsure, say you don't know.\n",
      "\n",
      "[USER]\n",
      "Question: What are the symptoms of Alpha-ketoglutarate dehydrogenase deficiency ?\n",
      "\n",
      "Follow this exact format:\n",
      "Reasoning:\n",
      "- bullet 1\n",
      "- bullet 2\n",
      "- bullet 3\n",
      "Final Answer: <one concise sentence>\n",
      "\n",
      "Be brief and avoid speculation.\n",
      "========================================================================================\n",
      "\n",
      "========================================================================================\n",
      "[CoT PROMPT]\n",
      "\n",
      "[SYSTEM]\n",
      "You are a concise, evidence-focused medical assistant. Reason step-by-step, then provide a final answer. If unsure, say you don't know.\n",
      "\n",
      "[USER]\n",
      "Question: What are the treatments for GLUT1 deficiency syndrome ?\n",
      "\n",
      "Follow this exact format:\n",
      "Reasoning:\n",
      "- bullet 1\n",
      "- bullet 2\n",
      "- bullet 3\n",
      "Final Answer: <one concise sentence>\n",
      "\n",
      "Be brief and avoid speculation.\n",
      "========================================================================================\n",
      "[CoT] Saved CoT traces to: ./medquad_cot_traces.csv  (rows=3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>gold</th>\n",
       "      <th>strategy</th>\n",
       "      <th>cot_reasoning</th>\n",
       "      <th>cot_final</th>\n",
       "      <th>answer</th>\n",
       "      <th>raw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Do you have information about X-Rays</td>\n",
       "      <td>Summary : X-rays are a type of radiation calle...</td>\n",
       "      <td>cot</td>\n",
       "      <td>Reasoning: - X‐rays produce images by passing ...</td>\n",
       "      <td>X‐rays are a medical imaging technique that us...</td>\n",
       "      <td>X‐rays are a medical imaging technique that us...</td>\n",
       "      <td>Reasoning: - X‐rays produce images by passing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are the symptoms of Alpha-ketoglutarate d...</td>\n",
       "      <td>What are the signs and symptoms of Alpha-ketog...</td>\n",
       "      <td>cot</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0               Do you have information about X-Rays   \n",
       "1  What are the symptoms of Alpha-ketoglutarate d...   \n",
       "\n",
       "                                                gold strategy  \\\n",
       "0  Summary : X-rays are a type of radiation calle...      cot   \n",
       "1  What are the signs and symptoms of Alpha-ketog...      cot   \n",
       "\n",
       "                                       cot_reasoning  \\\n",
       "0  Reasoning: - X‐rays produce images by passing ...   \n",
       "1                                                      \n",
       "\n",
       "                                           cot_final  \\\n",
       "0  X‐rays are a medical imaging technique that us...   \n",
       "1                                                      \n",
       "\n",
       "                                              answer  \\\n",
       "0  X‐rays are a medical imaging technique that us...   \n",
       "1                                                      \n",
       "\n",
       "                                                 raw  \n",
       "0  Reasoning: - X‐rays produce images by passing ...  \n",
       "1                                                     "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def run_cot(print_prompts: bool = False) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for _, r in eval_df.iterrows():\n",
    "        q   = normalize_text(str(r[\"question\"]))\n",
    "        gold= normalize_text(str(r[\"gold\"]))\n",
    "\n",
    "        msgs = build_cot_messages(q, print_prompt=print_prompts)\n",
    "        out  = chat_messages(GEN_MODEL, msgs, temperature=TEMPERATURE, max_tokens=MAX_TOKENS, top_p=TOP_P)\n",
    "\n",
    "        reasoning, final, raw = parse_cot(out)\n",
    "\n",
    "        rows.append({\n",
    "            \"question\": q,\n",
    "            \"gold\": gold,\n",
    "            \"strategy\": \"cot\",\n",
    "            \"cot_reasoning\": reasoning,\n",
    "            \"cot_final\": final,\n",
    "            \"answer\": final,    # scoring uses only the final answer\n",
    "            \"raw\": raw\n",
    "        })\n",
    "\n",
    "    df_cot = pd.DataFrame(rows)\n",
    "    out_csv = f\"{RESULTS_DIR}/medquad_cot_traces.csv\"\n",
    "    df_cot.to_csv(out_csv, index=False)\n",
    "    print(f\"[CoT] Saved CoT traces to: {out_csv}  (rows={len(df_cot)})\")\n",
    "    return df_cot\n",
    "\n",
    "cot_df = run_cot(print_prompts=PRINT_COT_PROMPTS)\n",
    "cot_df.head(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338d1fb4-3167-4907-a3cd-34eeb0f3fcf8",
   "metadata": {},
   "source": [
    "### LLM-as-judge metrics: Faithfulness, Hallucination, Answer Relevance, Answer Correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28688b75-85a9-4e00-93f5-148059bd0700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved CoT per-item scores to: ./medquad_cot_scores_improved.csv\n",
      "\n",
      "=== CoT Summary (averages) [IMPROVED] ===\n",
      "faithfulness             0.320\n",
      "hallucination_rate       0.680\n",
      "faithfulness_hard@0.5    0.333\n",
      "faithfulness_hard@0.8    0.333\n",
      "answer_relevance         0.750\n",
      "answer_correctness       0.755\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# === IMPROVED SCORING: soft faithfulness + hard@0.5/@0.8 ===\n",
    "import re, time, random\n",
    "import pandas as pd\n",
    "\n",
    "# judge settings (kept)\n",
    "JUDGE_TEMPERATURE = 0.0\n",
    "JUDGE_MAX_TOKENS  = 32\n",
    "JUDGE_TOP_P       = 1.0\n",
    "SHOW_JUDGE_SAMPLES = True\n",
    "MAX_RETRIES = 2\n",
    "\n",
    "# ---- numeric parsing unchanged ----\n",
    "def _parse_score_strict(txt: str) -> float:\n",
    "    if txt is None:\n",
    "        raise ValueError(\"empty judge output\")\n",
    "    s = txt.strip()\n",
    "    if re.fullmatch(r'(0(\\.\\d+)?|1(\\.0+)?)', s):\n",
    "        return float(s)\n",
    "    m = re.search(r'(?<![\\d.])(0(?:\\.\\d+)?|1(?:\\.0+)?|\\d\\.\\d+)(?![\\d.])', s)\n",
    "    if m:\n",
    "        val = float(m.group(1))\n",
    "        if 0.0 <= val <= 1.0:\n",
    "            return val\n",
    "    raise ValueError(f\"invalid judge output: {repr(txt)}\")\n",
    "\n",
    "def chat_judge_strict(system_prompt: str, user_prompt: str, model: str = JUDGE_MODEL) -> float:\n",
    "    last_txt = \"\"\n",
    "    for attempt in range(MAX_RETRIES):\n",
    "        r = client.chat.completions.create(\n",
    "            model=model, temperature=JUDGE_TEMPERATURE, max_tokens=JUDGE_MAX_TOKENS, top_p=JUDGE_TOP_P,\n",
    "            messages=[{\"role\":\"system\",\"content\":system_prompt},\n",
    "                      {\"role\":\"user\",\"content\":user_prompt}]\n",
    "        )\n",
    "        txt = r.choices[0].message.content.strip() if r.choices else \"\"\n",
    "        last_txt = txt\n",
    "        try:\n",
    "            return _parse_score_strict(txt)\n",
    "        except Exception:\n",
    "            system_prompt = \"Return ONLY a bare number in [0,1]. No words, no symbols, no extra text.\"\n",
    "            user_prompt = re.sub(r'\\s+', ' ', user_prompt)\n",
    "            time.sleep(0.3 * (attempt + 1))\n",
    "            continue\n",
    "    print(\"[judge warning] could not parse judge output after retries → returning 0.0; raw:\", repr(last_txt))\n",
    "    return 0.0\n",
    "\n",
    "# ---- metric calls ----\n",
    "def entail_prob(premise: str, claim: str) -> float:\n",
    "    sys = \"Return ONLY a bare number in [0,1] = P(premise entails claim). No words.\"\n",
    "    usr = f\"premise:\\n{premise}\\n\\nclaim:\\n{claim}\\n\\nnumber:\"\n",
    "    return chat_judge_strict(sys, usr, model=JUDGE_MODEL)\n",
    "\n",
    "def split_sents(t: str):\n",
    "    t = (t or \"\").strip()\n",
    "    sents = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', t) if s.strip()]\n",
    "    return sents if sents else ([t] if t else [])\n",
    "\n",
    "def faithfulness_metrics(answer: str, gold_reference: str, hard_thresh_list=(0.5, 0.8)):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      probs: list of entailment probs for each answer sentence\n",
    "      soft_faith: mean(probs)\n",
    "      hard_faith: dict {threshold -> fraction of sentences >= threshold}\n",
    "    \"\"\"\n",
    "    sents = split_sents(answer)\n",
    "    if not sents:\n",
    "        return [], 0.0, {th: 0.0 for th in hard_thresh_list}\n",
    "    probs = [entail_prob(gold_reference, s) for s in sents]\n",
    "    soft = sum(probs) / len(probs)\n",
    "    hard = {th: sum(p >= th for p in probs) / len(probs) for th in hard_thresh_list}\n",
    "    return probs, soft, hard\n",
    "\n",
    "def answer_correctness_llm(gold_answer: str, model_answer: str) -> float:\n",
    "    e1 = entail_prob(gold_answer, model_answer)\n",
    "    e2 = entail_prob(model_answer, gold_answer)\n",
    "    return 0.5 * (e1 + e2)\n",
    "\n",
    "def answer_relevance(q: str, a: str) -> float:\n",
    "    sys = \"Return ONLY a bare number in [0,1] = how well ANSWER addresses QUESTION. No words.\"\n",
    "    usr = f\"QUESTION:\\n{q}\\n\\nANSWER:\\n{a}\\n\\nnumber:\"\n",
    "    return chat_judge_strict(sys, usr, model=JUDGE_MODEL)\n",
    "\n",
    "def band(x: float) -> str:\n",
    "    return \"Excellent\" if x>=0.90 else \"Good\" if x>=0.75 else \"Borderline\" if x>=0.60 else \"Poor\"\n",
    "\n",
    "def score_cot_improved(df_answers: pd.DataFrame) -> pd.DataFrame:\n",
    "    # sanity: ensure answer != gold for most rows\n",
    "    same = (df_answers[\"answer\"].fillna(\"\").str.strip() == df_answers[\"gold\"].fillna(\"\").str.strip()).mean()\n",
    "    if same > 0.6:\n",
    "        print(f\"[sanity] {same:.0%} of rows have answer == gold; check your pipeline/columns.\")\n",
    "\n",
    "    rows = []\n",
    "    for _, r in df_answers.iterrows():\n",
    "        q, a, g = str(r[\"question\"]), str(r[\"answer\"]), str(r[\"gold\"])\n",
    "        probs, soft_faith, hard = faithfulness_metrics(a, g, hard_thresh_list=(0.5, 0.8))\n",
    "        relev = answer_relevance(q, a)\n",
    "        corr  = answer_correctness_llm(g, a)\n",
    "\n",
    "        rows.append({\n",
    "            \"question\": q,\n",
    "            \"strategy\": \"cot\",\n",
    "            # primary faithfulness now uses SOFT mean:\n",
    "            \"faithfulness\": round(soft_faith, 3),\n",
    "            \"hallucination_rate\": round(1.0 - soft_faith, 3),\n",
    "            # keep hard variants for transparency:\n",
    "            \"faithfulness_hard@0.5\": round(hard[0.5], 3),\n",
    "            \"faithfulness_hard@0.8\": round(hard[0.8], 3),\n",
    "            \"answer_relevance\": round(relev, 3),\n",
    "            \"answer_correctness\": round(corr, 3),\n",
    "            \"faith_band\": band(soft_faith),\n",
    "            \"relevance_band\": band(relev),\n",
    "            \"correctness_band\": band(corr),\n",
    "            # optional: diagnostic stats\n",
    "            \"faith_sentences\": len(probs),\n",
    "            \"faith_min\": round(min(probs), 3) if probs else None,\n",
    "            \"faith_max\": round(max(probs), 3) if probs else None,\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# ---- run improved scoring on your CoT outputs ----\n",
    "cot_scored = score_cot_improved(cot_df)\n",
    "cot_scored.to_csv(f\"{RESULTS_DIR}/medquad_cot_scores_improved.csv\", index=False)\n",
    "print(\"Saved CoT per-item scores to:\", f\"{RESULTS_DIR}/medquad_cot_scores_improved.csv\")\n",
    "\n",
    "print(\"\\n=== CoT Summary (averages) [IMPROVED] ===\")\n",
    "summary = cot_scored[[\"faithfulness\",\"hallucination_rate\",\"faithfulness_hard@0.5\",\"faithfulness_hard@0.8\",\"answer_relevance\",\"answer_correctness\"]].mean().round(3)\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fda2893-3f42-4623-b107-d21c03afa0fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa07ed99-c138-4a83-b43e-018e076189c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
