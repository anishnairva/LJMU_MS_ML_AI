{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d833b64-6039-497d-a4b3-5c076f08a5b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ragas==0.1.9 in c:\\users\\anish\\anaconda3\\lib\\site-packages (0.1.9)\n",
      "Requirement already satisfied: datasets==2.20.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (2.20.0)\n",
      "Requirement already satisfied: pyarrow==17.0.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (17.0.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\anish\\anaconda3\\lib\\site-packages (from ragas==0.1.9) (1.26.4)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\anish\\anaconda3\\lib\\site-packages (from ragas==0.1.9) (0.9.0)\n",
      "Requirement already satisfied: langchain in c:\\users\\anish\\anaconda3\\lib\\site-packages (from ragas==0.1.9) (0.3.27)\n",
      "Requirement already satisfied: langchain-core in c:\\users\\anish\\anaconda3\\lib\\site-packages (from ragas==0.1.9) (0.3.74)\n",
      "Requirement already satisfied: langchain-community in c:\\users\\anish\\anaconda3\\lib\\site-packages (from ragas==0.1.9) (0.3.27)\n",
      "Requirement already satisfied: langchain-openai in c:\\users\\anish\\anaconda3\\lib\\site-packages (from ragas==0.1.9) (0.3.29)\n",
      "Requirement already satisfied: openai>1 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from ragas==0.1.9) (1.99.7)\n",
      "Requirement already satisfied: pysbd>=0.3.4 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from ragas==0.1.9) (0.3.4)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\anish\\anaconda3\\lib\\site-packages (from ragas==0.1.9) (1.6.0)\n",
      "Requirement already satisfied: appdirs in c:\\users\\anish\\anaconda3\\lib\\site-packages (from ragas==0.1.9) (1.4.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\anish\\anaconda3\\lib\\site-packages (from datasets==2.20.0) (3.13.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in c:\\users\\anish\\anaconda3\\lib\\site-packages (from datasets==2.20.0) (0.7)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from datasets==2.20.0) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\anish\\anaconda3\\lib\\site-packages (from datasets==2.20.0) (2.1.4)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from datasets==2.20.0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from datasets==2.20.0) (4.66.5)\n",
      "Requirement already satisfied: xxhash in c:\\users\\anish\\anaconda3\\lib\\site-packages (from datasets==2.20.0) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\anish\\anaconda3\\lib\\site-packages (from datasets==2.20.0) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets==2.20.0) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\anish\\anaconda3\\lib\\site-packages (from datasets==2.20.0) (3.9.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from datasets==2.20.0) (0.34.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\anish\\anaconda3\\lib\\site-packages (from datasets==2.20.0) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from datasets==2.20.0) (6.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from aiohttp->datasets==2.20.0) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from aiohttp->datasets==2.20.0) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from aiohttp->datasets==2.20.0) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from aiohttp->datasets==2.20.0) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from aiohttp->datasets==2.20.0) (1.9.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.21.2->datasets==2.20.0) (4.12.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from openai>1->ragas==0.1.9) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from openai>1->ragas==0.1.9) (1.8.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from openai>1->ragas==0.1.9) (0.27.2)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from openai>1->ragas==0.1.9) (0.9.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from openai>1->ragas==0.1.9) (2.8.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\anish\\anaconda3\\lib\\site-packages (from openai>1->ragas==0.1.9) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets==2.20.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets==2.20.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets==2.20.0) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets==2.20.0) (2025.1.31)\n",
      "Requirement already satisfied: colorama in c:\\users\\anish\\anaconda3\\lib\\site-packages (from tqdm>=4.66.3->datasets==2.20.0) (0.4.6)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from langchain->ragas==0.1.9) (0.3.9)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from langchain->ragas==0.1.9) (0.4.13)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from langchain->ragas==0.1.9) (2.0.42)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from langchain-core->ragas==0.1.9) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from langchain-core->ragas==0.1.9) (1.33)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from langchain-community->ragas==0.1.9) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from langchain-community->ragas==0.1.9) (2.10.1)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from langchain-community->ragas==0.1.9) (0.4.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from tiktoken->ragas==0.1.9) (2023.10.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from pandas->datasets==2.20.0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from pandas->datasets==2.20.0) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from pandas->datasets==2.20.0) (2023.3)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->ragas==0.1.9) (3.22.0)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->ragas==0.1.9) (0.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\anish\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai>1->ragas==0.1.9) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>1->ragas==0.1.9) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core->ragas==0.1.9) (2.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from langsmith>=0.1.17->langchain->ragas==0.1.9) (3.10.7)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from langsmith>=0.1.17->langchain->ragas==0.1.9) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from langsmith>=0.1.17->langchain->ragas==0.1.9) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->openai>1->ragas==0.1.9) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->openai>1->ragas==0.1.9) (2.20.1)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community->ragas==0.1.9) (1.1.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community->ragas==0.1.9) (0.4.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.20.0) (1.16.0)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain->ragas==0.1.9) (3.0.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community->ragas==0.1.9) (1.0.0)\n",
      "Collecting langchain==0.3.2\n",
      "  Using cached langchain-0.3.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting langchain-community==0.3.2\n",
      "  Using cached langchain_community-0.3.2-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting langchain-huggingface==0.1.0\n",
      "  Using cached langchain_huggingface-0.1.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from langchain==0.3.2) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from langchain==0.3.2) (2.0.42)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from langchain==0.3.2) (3.9.3)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.8 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from langchain==0.3.2) (0.3.74)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from langchain==0.3.2) (0.3.9)\n",
      "Collecting langsmith<0.2.0,>=0.1.17 (from langchain==0.3.2)\n",
      "  Using cached langsmith-0.1.147-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from langchain==0.3.2) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from langchain==0.3.2) (2.8.2)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from langchain==0.3.2) (2.32.3)\n",
      "Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain==0.3.2)\n",
      "  Using cached tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from langchain-community==0.3.2) (0.6.7)\n",
      "INFO: pip is looking at multiple versions of langchain-community to determine which version is compatible with other requirements. This could take a while.\n",
      "\n",
      "The conflict is caused by:\n",
      "    The user requested langchain==0.3.2\n",
      "    langchain-community 0.3.2 depends on langchain<0.4.0 and >=0.3.3\n",
      "\n",
      "To fix this you could try to:\n",
      "1. loosen the range of package versions you've specified\n",
      "2. remove package versions to allow pip attempt to solve the dependency conflict\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Cannot install langchain-community==0.3.2 and langchain==0.3.2 because these package versions have conflicting dependencies.\n",
      "ERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tokenizers==0.19.1 in c:\\users\\anish\\anaconda3\\lib\\site-packages (0.19.1)\n",
      "Requirement already satisfied: sentence-transformers==3.0.1 in c:\\users\\anish\\anaconda3\\lib\\site-packages (3.0.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from tokenizers==0.19.1) (0.34.3)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from sentence-transformers==3.0.1) (4.44.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\anish\\anaconda3\\lib\\site-packages (from sentence-transformers==3.0.1) (4.66.5)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from sentence-transformers==3.0.1) (2.5.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\anish\\anaconda3\\lib\\site-packages (from sentence-transformers==3.0.1) (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\anish\\anaconda3\\lib\\site-packages (from sentence-transformers==3.0.1) (1.4.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\anish\\anaconda3\\lib\\site-packages (from sentence-transformers==3.0.1) (1.11.4)\n",
      "Requirement already satisfied: Pillow in c:\\users\\anish\\anaconda3\\lib\\site-packages (from sentence-transformers==3.0.1) (11.3.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\anish\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (2023.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (6.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\anish\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (4.12.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers==3.0.1) (1.13.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\anish\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers==3.0.1) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers==3.0.1) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers==3.0.1) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\anish\\anaconda3\\lib\\site-packages (from tqdm->sentence-transformers==3.0.1) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers==3.0.1) (2023.10.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers==3.0.1) (0.4.5)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers==3.0.1) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers==3.0.1) (2.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers==3.0.1) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (2025.1.31)\n",
      "Requirement already satisfied: nest_asyncio in c:\\users\\anish\\anaconda3\\lib\\site-packages (1.6.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Skipping intel-openmp as it is not installed.\n"
     ]
    }
   ],
   "source": [
    "!pip install -U ragas==0.1.9 datasets==2.20.0 pyarrow==17.0.0\n",
    "!pip install -U langchain==0.3.2 langchain-community==0.3.2 langchain-huggingface==0.1.0\n",
    "!pip install -U tokenizers==0.19.1 sentence-transformers==3.0.1\n",
    "!pip install -U nest_asyncio\n",
    "# optional but recommended on Windows to avoid MKL/OpenMP clashes:\n",
    "!pip uninstall -y intel-openmp || true\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1f63548-692a-4081-81f7-e60575402392",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'false'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, platform\n",
    "# Tame OpenMP/tokenizers threading & duplication issues on Windows\n",
    "os.environ.setdefault(\"KMP_DUPLICATE_LIB_OK\", \"TRUE\")        # avoids OpenMP hard crash\n",
    "os.environ.setdefault(\"OMP_NUM_THREADS\", \"1\")                # prevents oversubscription\n",
    "os.environ.setdefault(\"TOKENIZERS_PARALLELISM\", \"false\")     # quieter tokenizers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5111e0a7-a4ac-4d59-b7f4-8188eb0b244b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Config =====\n",
    "DATA_PATH   = \"medqa_cleaned.csv\"  # your cleaned CSV (unchanged)\n",
    "ANSWER_FIELD = \"answer\"                         # <- ground truth column in your CSV (DO NOT RENAME)\n",
    "TEXT_FIELD   = \"answer\"                         # <- what to index for retrieval (use \"source_text\" if you have it)\n",
    "\n",
    "TOP_K = 3\n",
    "N_EVAL = 3  # same 3 questions as prompting (use selection CSV below)\n",
    "PRINT_PROMPTS = True\n",
    "\n",
    "GEN_MODEL   = \"openai/gpt-oss-20b\"\n",
    "GROQ_API_KEY = \"\"\n",
    "\n",
    "# Use the EXACT same selection file from the prompt run (support both 'gold' or 'answer' inside it)\n",
    "EVAL_SELECTION_CSV = \"medquad_selected_questions.csv\"\n",
    "\n",
    "#RAGAS_EMBED_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "SEED = 42\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82f179e-6f72-4e03-aba3-ab4b3f90357d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ebf58cc-15f8-4b60-9ca8-25b803d756d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded rows: 16018 | columns: ['question', 'answer', 'qtype', 'doc_id']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>qtype</th>\n",
       "      <th>doc_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Who is at risk for Lymphocytic Choriomeningiti...</td>\n",
       "      <td>LCMV infections can occur after exposure to fr...</td>\n",
       "      <td>susceptibility</td>\n",
       "      <td>doc_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are the symptoms of Lymphocytic Choriomen...</td>\n",
       "      <td>LCMV is most commonly recognized as causing ne...</td>\n",
       "      <td>symptoms</td>\n",
       "      <td>doc_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Who is at risk for Lymphocytic Choriomeningiti...</td>\n",
       "      <td>Individuals of all ages who come into contact ...</td>\n",
       "      <td>susceptibility</td>\n",
       "      <td>doc_2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  Who is at risk for Lymphocytic Choriomeningiti...   \n",
       "1  What are the symptoms of Lymphocytic Choriomen...   \n",
       "2  Who is at risk for Lymphocytic Choriomeningiti...   \n",
       "\n",
       "                                              answer           qtype doc_id  \n",
       "0  LCMV infections can occur after exposure to fr...  susceptibility  doc_0  \n",
       "1  LCMV is most commonly recognized as causing ne...        symptoms  doc_1  \n",
       "2  Individuals of all ages who come into contact ...  susceptibility  doc_2  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "# sanity checks (no column renames here)\n",
    "assert \"question\" in df.columns, \"CSV must have a 'question' column\"\n",
    "assert ANSWER_FIELD in df.columns, f\"CSV must have the ground-truth column '{ANSWER_FIELD}'\"\n",
    "assert TEXT_FIELD   in df.columns, f\"CSV must have the text-to-index column '{TEXT_FIELD}'\"\n",
    "\n",
    "# stable IDs without altering source columns\n",
    "if \"doc_id\" not in df.columns:\n",
    "    df = df.copy()\n",
    "    df[\"doc_id\"] = [f\"doc_{i}\" for i in range(len(df))]\n",
    "\n",
    "# light normalization (safe)\n",
    "df[\"question\"] = df[\"question\"].astype(str).str.strip()\n",
    "df[ANSWER_FIELD] = df[ANSWER_FIELD].astype(str).str.strip()\n",
    "df[TEXT_FIELD]   = df[TEXT_FIELD].astype(str).str.strip()\n",
    "\n",
    "print(\"Loaded rows:\", len(df), \"| columns:\", list(df.columns))\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37c6d7d-1c87-4692-90bc-6571d653a750",
   "metadata": {},
   "source": [
    "## BM25 index over TEXT_FIELD (not touching answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6895ec31-f291-4a21-aa75-7a86ecf5b686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rank_bm25 in c:\\users\\anish\\anaconda3\\lib\\site-packages (0.2.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\anish\\anaconda3\\lib\\site-packages (from rank_bm25) (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install rank_bm25 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aab28216-0ea4-4c2d-8d23-3e02694640ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM25 built over 16018 docs (TEXT_FIELD='answer').\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "def simple_tokenize(text: str):\n",
    "    return re.findall(r\"[A-Za-z0-9']+\", (text or \"\").lower())\n",
    "\n",
    "documents = df[TEXT_FIELD].tolist()   # <- whatever you chose to index\n",
    "doc_ids   = df[\"doc_id\"].tolist()\n",
    "\n",
    "tokenized_docs = [simple_tokenize(t) for t in documents]\n",
    "bm25 = BM25Okapi(tokenized_docs)\n",
    "print(f\"BM25 built over {len(tokenized_docs)} docs (TEXT_FIELD='{TEXT_FIELD}').\")\n",
    "\n",
    "def bm25_retrieve(query: str, k: int = 3):\n",
    "    q_tokens = simple_tokenize(query)\n",
    "    scores = bm25.get_scores(q_tokens)\n",
    "    top_idx = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:k]\n",
    "    return [(doc_ids[i], documents[i], float(scores[i])) for i in top_idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a37cefe-6835-4196-8926-309d5730e04f",
   "metadata": {},
   "source": [
    "##  Use the same 3 questions as prompting (support both gold or answer in the selection file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7b3bf54-ecdc-4484-861a-83f8236f987b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating SAME questions used in prompting: 3\n",
      "              Do you have information about X-Rays\n",
      "What are the symptoms of Alpha-ketoglutarate de...\n",
      "What are the treatments for GLUT1 deficiency sy...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "sel = pd.read_csv(EVAL_SELECTION_CSV)\n",
    "\n",
    "# figure out which column in the selection file holds the gold text\n",
    "sel_gold_col = \"gold\" if \"gold\" in sel.columns else (ANSWER_FIELD if ANSWER_FIELD in sel.columns else None)\n",
    "assert sel_gold_col is not None, f\"Selection file must contain either 'gold' or '{ANSWER_FIELD}'\"\n",
    "\n",
    "eval_df = sel[[\"question\", sel_gold_col]].copy()\n",
    "eval_df = eval_df.head(min(N_EVAL, len(eval_df)))  # keep same 3 if that's what you saved\n",
    "print(\"Evaluating SAME questions used in prompting:\", len(eval_df))\n",
    "print(eval_df[\"question\"].to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575e3ba3-b539-4b0e-8ea1-ff584710a8ac",
   "metadata": {},
   "source": [
    "### Prompt builder + Groq call (unchanged, prints full prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45434a52-6126-4a47-bfe6-1a00a323e2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "\n",
    "client = Groq(api_key=GROQ_API_KEY)\n",
    "\n",
    "def build_rag_messages(question: str, contexts: list[str], print_prompt=True):\n",
    "    system_txt = (\n",
    "        \"You are a concise, evidence-focused medical assistant. \"\n",
    "        \"Use the provided context passages to answer accurately. \"\n",
    "        \"If the context does not contain the answer, say you don't know.\"\n",
    "    )\n",
    "    ctx_block = \"\\n\\n\".join([f\"[Context {i+1}]\\n{c}\" for i, c in enumerate(contexts)])\n",
    "    user_txt = (\n",
    "        f\"Question: {question}\\n\\n\"\n",
    "        f\"Context Passages:\\n{ctx_block}\\n\\n\"\n",
    "        \"Instructions: Answer in 2–4 sentences. Cite which Context numbers support your statements (e.g., [1], [2]). \"\n",
    "        \"If insufficient evidence, say 'I don't know based on the given context.'\"\n",
    "    )\n",
    "    messages = [{\"role\":\"system\",\"content\":system_txt},{\"role\":\"user\",\"content\":user_txt}]\n",
    "    if print_prompt:\n",
    "        print(\"\\n\"+\"=\"*88); print(\"[RAG PROMPT]\")\n",
    "        print(\"\\n[SYSTEM]\\n\"+system_txt)\n",
    "        print(\"\\n[USER]\\n\"+user_txt)\n",
    "        print(\"=\"*88)\n",
    "    return messages\n",
    "\n",
    "def groq_chat(messages, model=GEN_MODEL, temperature=0.0, max_tokens=512, top_p=1.0):\n",
    "    r = client.chat.completions.create(\n",
    "        model=model, temperature=temperature, max_tokens=max_tokens, top_p=top_p, messages=messages\n",
    "    )\n",
    "    return r.choices[0].message.content.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b785de0e-dee1-4bf6-a21f-63fb3067731d",
   "metadata": {},
   "source": [
    "### Run RAG on those questions (use original answer column as ground truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3dc6cbf9-3f07-48d2-a091-20c297f07f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================================================================\n",
      "[RAG PROMPT]\n",
      "\n",
      "[SYSTEM]\n",
      "You are a concise, evidence-focused medical assistant. Use the provided context passages to answer accurately. If the context does not contain the answer, say you don't know.\n",
      "\n",
      "[USER]\n",
      "Question: Do you have information about X-Rays\n",
      "\n",
      "Context Passages:\n",
      "[Context 1]\n",
      "Summary : X-rays are a type of radiation called electromagnetic waves. X-ray imaging creates pictures of the inside of your body. The images show the parts of your body in different shades of black and white. This is because different tissues absorb different amounts of radiation. Calcium in bones absorbs x-rays the most, so bones look white. Fat and other soft tissues absorb less, and look gray. Air absorbs the least, so lungs look black. The most familiar use of x-rays is checking for broken bones, but x-rays are also used in other ways. For example, chest x-rays can spot pneumonia. Mammograms use x-rays to look for breast cancer. When you have an x-ray, you may wear a lead apron to protect certain parts of your body. The amount of radiation you get from an x-ray is small. For example, a chest x-ray gives out a radiation dose similar to the amount of radiation you're naturally exposed to from the environment over 10 days.\n",
      "\n",
      "[Context 2]\n",
      "An Underdiagnosed Disease Experts believe that Paget's disease is underdiagnosed; people with a mild case and no symptoms may never know they have the disease. Or, they may receive a diagnosis by accident when x-rays or other laboratory tests done for another reason reveal Paget's disease. When symptoms do occur, they usually appear gradually and, in the early stages, may be confused with those of arthritis or other medical problems. Sometimes a person may not receive a clear diagnosis until the disease progresses and complications develop. Diagnostic Tests X-rays are almost always used to diagnose Paget's disease, but the disease may be discovered using one of three tests: - x-rays - an alkaline phosphatase blood test - or a bone scan. x-rays an alkaline phosphatase blood test or a bone scan. Bones affected by Paget's disease have a distinctive appearance on x-rays, which may show increased bone density, an abnormal bone structure, bowing, and enlargement. X-rays of leg bones may show very tiny fractures called microfractures. The enzyme alkaline phosphatase is involved in the normal growth of new bone. Having higher-than-normal levels of this chemical in the blood, however, may be a sign of Paget's disease. The alkaline phosphatase blood test measures the level of this substance. A bone scan provides a picture of the affected bones that doctors can use to see how far the disease has progressed. If a bone scan done for another reason suggests Paget's disease, the doctor can order x-rays to confirm the diagnosis. If the Disease Runs in the Family Early diagnosis and treatment of Paget's disease is important. Because Paget's disease can be hereditary, some experts recommend that the brothers, sisters, and children of anyone with the disease talk to their doctor about having an alkaline phosphatase blood test every 2 to 3 years after about age 40.\n",
      "\n",
      "[Context 3]\n",
      "How is amelogenesis imperfecta diagnosed? A dentist can identify and diagnose amelogenesis imperfecta on the basis of the patient's family history and the signs and symptoms present in the affected individual. Extraoral X-rays (X-rays taken outside the mouth) can reveal the presence of teeth that never erupted o that were absorbed. Intraoral X-rays (X-rays taken inside the mouth) show contrast between the enamel and dentin in cases in which mineralization is affected. Genetic testing is available for the genes AMELX, ENAM, and MMP20. You can visit the Genetic Testing Registry to locate laboratories performing genetic testing for these genes. The American Academy of Pediatric Dentistry is a source of information to find a pediatric dentist. The National Dental Association can also assist people in locating a dentist.\n",
      "\n",
      "Instructions: Answer in 2–4 sentences. Cite which Context numbers support your statements (e.g., [1], [2]). If insufficient evidence, say 'I don't know based on the given context.'\n",
      "========================================================================================\n",
      "\n",
      "========================================================================================\n",
      "[RAG PROMPT]\n",
      "\n",
      "[SYSTEM]\n",
      "You are a concise, evidence-focused medical assistant. Use the provided context passages to answer accurately. If the context does not contain the answer, say you don't know.\n",
      "\n",
      "[USER]\n",
      "Question: What are the symptoms of Alpha-ketoglutarate dehydrogenase deficiency ?\n",
      "\n",
      "Context Passages:\n",
      "[Context 1]\n",
      "What are the signs and symptoms of Alpha-ketoglutarate dehydrogenase deficiency? The Human Phenotype Ontology provides the following list of signs and symptoms for Alpha-ketoglutarate dehydrogenase deficiency. If the information is available, the table below includes how often the symptom is seen in people with this condition. You can use the MedlinePlus Medical Dictionary to look up the definitions for these medical terms. Signs and Symptoms Approximate number of patients (when available) Cognitive impairment 90% Hypertonia 90% Incoordination 90% Short stature 90% Skeletal muscle atrophy 90% Abnormality of movement 50% Abnormality of the salivary glands 50% Hydrocephalus 50% Autosomal recessive inheritance - Congenital lactic acidosis - Death in childhood - Increased serum lactate - Metabolic acidosis - Muscular hypotonia - The Human Phenotype Ontology (HPO) has collected information on how often a sign or symptom occurs in a condition. Much of this information comes from Orphanet, a European rare disease database. The frequency of a sign or symptom is usually listed as a rough estimate of the percentage of patients who have that feature. The frequency may also be listed as a fraction. The first number of the fraction is how many people had the symptom, and the second number is the total number of people who were examined in one study. For example, a frequency of 25/25 means that in a study of 25 people all patients were found to have that symptom. Because these frequencies are based on a specific study, the fractions may be different if another group of patients are examined. Sometimes, no information on frequency is available. In these cases, the sign or symptom may be rare or common.\n",
      "\n",
      "[Context 2]\n",
      "Mutations in the DLD gene cause dihydrolipoamide dehydrogenase deficiency. This gene provides instructions for making an enzyme called dihydrolipoamide dehydrogenase (DLD). DLD is one component of three different groups of enzymes that work together (enzyme complexes): branched-chain alpha-keto acid dehydrogenase (BCKD), pyruvate dehydrogenase (PDH), and alpha ()-ketoglutarate dehydrogenase (KGDH). The BCKD enzyme complex is involved in the breakdown of three protein building blocks (amino acids) commonly found in protein-rich foods: leucine, isoleucine, and valine. Breakdown of these amino acids produces molecules that can be used for energy. The PDH and KGDH enzyme complexes are involved in other reactions in the pathways that convert the energy from food into a form that cells can use. Mutations in the DLD gene impair the function of the DLD enzyme, which prevents the three enzyme complexes from functioning properly. As a result, molecules that are normally broken down and their byproducts build up in the body, damaging tissues and leading to lactic acidosis and other chemical imbalances. In addition, the production of cellular energy is diminished. The brain is especially affected by the buildup of molecules and the lack of cellular energy, resulting in the neurological problems associated with dihydrolipoamide dehydrogenase deficiency. Liver problems are likely also related to decreased energy production in cells. The degree of impairment of each complex contributes to the variability in the features of this condition.\n",
      "\n",
      "[Context 3]\n",
      "Mutations in the SLC25A19 gene cause Amish lethal microcephaly. The SLC25A19 gene provides instructions for producing a protein that is a member of the solute carrier (SLC) family of proteins. Proteins in the SLC family transport various compounds across the membranes surrounding the cell and its component parts. The protein produced from the SLC25A19 gene transports a molecule called thiamine pyrophosphate into the mitochondria, the energy-producing centers of cells. This compound is involved in the activity of a group of mitochondrial enzymes called the dehydrogenase complexes, one of which is the alpha-ketoglutarate dehydrogenase complex. The transport of thiamine pyrophosphate into the mitochondria is believed to be important in brain development. All known individuals with Amish lethal microcephaly have a mutation in which the protein building block (amino acid) alanine is substituted for the amino acid glycine at position 177 of the SLC25A19 protein, written as Gly177Ala or G177A. Researchers believe that this mutation interferes with the transport of thiamine pyrophosphate into the mitochondria and the activity of the alpha-ketoglutarate dehydrogenase complex, resulting in the abnormal brain development and alpha-ketoglutaric aciduria seen in Amish lethal microcephaly.\n",
      "\n",
      "Instructions: Answer in 2–4 sentences. Cite which Context numbers support your statements (e.g., [1], [2]). If insufficient evidence, say 'I don't know based on the given context.'\n",
      "========================================================================================\n",
      "\n",
      "========================================================================================\n",
      "[RAG PROMPT]\n",
      "\n",
      "[SYSTEM]\n",
      "You are a concise, evidence-focused medical assistant. Use the provided context passages to answer accurately. If the context does not contain the answer, say you don't know.\n",
      "\n",
      "[USER]\n",
      "Question: What are the treatments for GLUT1 deficiency syndrome ?\n",
      "\n",
      "Context Passages:\n",
      "[Context 1]\n",
      "Glucose transporter type 1 deficiency syndrome (GLUT1 deficiency syndrome) is an inherited condition that affects the nervous system. Signs and symptoms generally develop within the first few months of life and may include recurrent seizures (epilepsy) and involuntary eye movements. Affected people may also have microcephaly (unusually small head size) that develops after birth, developmental delay, intellectual disability and other neurological problems such as spasticity, ataxia (difficulty coordinating movements), and dysarthria. Approximately 10% of affected people have the \"non-epileptic\" form of GLUT1 deficiency syndrome which is associated with all the typical symptoms of the condition without seizures. GLUT1 deficiency syndrome is caused by changes (mutations) in the SLC2A1 gene and is inherited in an autosomal dominant manner. Although there is currently no cure for GLUT1 deficiency syndrome, a special diet (called a ketogenic diet) may help alleviate symptoms.\n",
      "\n",
      "[Context 2]\n",
      "How might glucose transporter type 1 deficiency syndrome be treated? There is currently no cure for glucose transporter type 1 deficiency syndrome (GLUT1 deficiency syndrome); however, a special diet (called a ketogenic diet) may help control symptoms in some affected people. The GLUT1 Deficiency Foundation offers an information page with detailed information regarding the ketogenic diet. Please click on the link to access this resource.\n",
      "\n",
      "[Context 3]\n",
      "GLUT1 deficiency syndrome is a disorder affecting the nervous system that can have a variety of neurological signs and symptoms. Approximately 90 percent of affected individuals have a form of the disorder often referred to as common GLUT1 deficiency syndrome. These individuals generally have frequent seizures (epilepsy) beginning in the first months of life. In newborns, the first sign of the disorder may be involuntary eye movements that are rapid and irregular. Babies with common GLUT1 deficiency syndrome have a normal head size at birth, but growth of the brain and skull is often slow, which can result in an abnormally small head size (microcephaly). People with this form of GLUT1 deficiency syndrome may have developmental delay or intellectual disability. Most affected individuals also have other neurological problems, such as stiffness caused by abnormal tensing of the muscles (spasticity), difficulty in coordinating movements (ataxia), and speech difficulties (dysarthria). Some experience episodes of confusion, lack of energy (lethargy), headaches, or muscle twitches (myoclonus), particularly during periods without food (fasting). About 10 percent of individuals with GLUT1 deficiency syndrome have a form of the disorder often known as non-epileptic GLUT1 deficiency syndrome, which is usually less severe than the common form. People with the non-epileptic form do not have seizures, but they may still have developmental delay and intellectual disability. Most have movement problems such as ataxia or involuntary tensing of various muscles (dystonia); the movement problems may be more pronounced than in the common form. Several conditions that were originally given other names have since been recognized to be variants of GLUT1 deficiency syndrome. These include paroxysmal choreoathetosis with spasticity (dystonia 9); paroxysmal exercise-induced dyskinesia and epilepsy (dystonia 18); and certain types of epilepsy. In rare cases, people with variants of GLUT1 deficiency syndrome produce abnormal red blood cells and have uncommon forms of a blood condition known as anemia, which is characterized by a shortage of red blood cells.\n",
      "\n",
      "Instructions: Answer in 2–4 sentences. Cite which Context numbers support your statements (e.g., [1], [2]). If insufficient evidence, say 'I don't know based on the given context.'\n",
      "========================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>contexts</th>\n",
       "      <th>ground_truth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Do you have information about X-Rays</td>\n",
       "      <td>Yes, X‑rays are a form of electromagnetic radi...</td>\n",
       "      <td>[Summary : X-rays are a type of radiation call...</td>\n",
       "      <td>Summary : X-rays are a type of radiation calle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are the symptoms of Alpha-ketoglutarate d...</td>\n",
       "      <td>Alpha‑ketoglutarate dehydrogenase deficiency i...</td>\n",
       "      <td>[What are the signs and symptoms of Alpha-keto...</td>\n",
       "      <td>What are the signs and symptoms of Alpha-ketog...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0               Do you have information about X-Rays   \n",
       "1  What are the symptoms of Alpha-ketoglutarate d...   \n",
       "\n",
       "                                              answer  \\\n",
       "0  Yes, X‑rays are a form of electromagnetic radi...   \n",
       "1  Alpha‑ketoglutarate dehydrogenase deficiency i...   \n",
       "\n",
       "                                            contexts  \\\n",
       "0  [Summary : X-rays are a type of radiation call...   \n",
       "1  [What are the signs and symptoms of Alpha-keto...   \n",
       "\n",
       "                                        ground_truth  \n",
       "0  Summary : X-rays are a type of radiation calle...  \n",
       "1  What are the signs and symptoms of Alpha-ketog...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows = []\n",
    "for _, r in eval_df.iterrows():\n",
    "    q = str(r[\"question\"]).strip()\n",
    "    gold_text = str(r[sel_gold_col]).strip()  # keep whatever the selection file provided\n",
    "\n",
    "    hits = bm25_retrieve(q, k=TOP_K)\n",
    "    contexts = [text for (_id, text, _score) in hits]\n",
    "\n",
    "    messages = build_rag_messages(q, contexts, print_prompt=PRINT_PROMPTS)\n",
    "    ans = groq_chat(messages, model=GEN_MODEL, temperature=0.0, max_tokens=512, top_p=1.0)\n",
    "\n",
    "    rows.append({\n",
    "        \"question\": q,\n",
    "        \"answer\": ans,\n",
    "        \"contexts\": contexts,\n",
    "        \"ground_truth\": gold_text   # RAGAS expects this name; we DO NOT rename the CSV column itself\n",
    "    })\n",
    "\n",
    "rag_results_df = pd.DataFrame(rows)\n",
    "rag_results_df.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e917b83-2a2a-4bf5-aa6d-a20761b5ad80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Summary : X-rays are a type of radiation called electromagnetic waves. X-ray imaging creates pictures of the inside of your body. The images show the parts of your body in different shades of black and white. This is because different tissues absorb different amounts of radiation. Calcium in bones absorbs x-rays the most, so bones look white. Fat and other soft tissues absorb less, and look gray. Air absorbs the least, so lungs look black. The most familiar use of x-rays is checking for broken bones, but x-rays are also used in other ways. For example, chest x-rays can spot pneumonia. Mammograms use x-rays to look for breast cancer. When you have an x-ray, you may wear a lead apron to protect certain parts of your body. The amount of radiation you get from an x-ray is small. For example, a chest x-ray gives out a radiation dose similar to the amount of radiation you're naturally exposed to from the environment over 10 days.\", \"An Underdiagnosed Disease Experts believe that Paget's disease is underdiagnosed; people with a mild case and no symptoms may never know they have the disease. Or, they may receive a diagnosis by accident when x-rays or other laboratory tests done for another reason reveal Paget's disease. When symptoms do occur, they usually appear gradually and, in the early stages, may be confused with those of arthritis or other medical problems. Sometimes a person may not receive a clear diagnosis until the disease progresses and complications develop. Diagnostic Tests X-rays are almost always used to diagnose Paget's disease, but the disease may be discovered using one of three tests: - x-rays - an alkaline phosphatase blood test - or a bone scan. x-rays an alkaline phosphatase blood test or a bone scan. Bones affected by Paget's disease have a distinctive appearance on x-rays, which may show increased bone density, an abnormal bone structure, bowing, and enlargement. X-rays of leg bones may show very tiny fractures called microfractures. The enzyme alkaline phosphatase is involved in the normal growth of new bone. Having higher-than-normal levels of this chemical in the blood, however, may be a sign of Paget's disease. The alkaline phosphatase blood test measures the level of this substance. A bone scan provides a picture of the affected bones that doctors can use to see how far the disease has progressed. If a bone scan done for another reason suggests Paget's disease, the doctor can order x-rays to confirm the diagnosis. If the Disease Runs in the Family Early diagnosis and treatment of Paget's disease is important. Because Paget's disease can be hereditary, some experts recommend that the brothers, sisters, and children of anyone with the disease talk to their doctor about having an alkaline phosphatase blood test every 2 to 3 years after about age 40.\", \"How is amelogenesis imperfecta diagnosed? A dentist can identify and diagnose amelogenesis imperfecta on the basis of the patient's family history and the signs and symptoms present in the affected individual. Extraoral X-rays (X-rays taken outside the mouth) can reveal the presence of teeth that never erupted o that were absorbed. Intraoral X-rays (X-rays taken inside the mouth) show contrast between the enamel and dentin in cases in which mineralization is affected. Genetic testing is available for the genes AMELX, ENAM, and MMP20. You can visit the Genetic Testing Registry to locate laboratories performing genetic testing for these genes. The American Academy of Pediatric Dentistry is a source of information to find a pediatric dentist. The National Dental Association can also assist people in locating a dentist.\"]\n"
     ]
    }
   ],
   "source": [
    "print(rag_results_df.iloc[0, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a61a0907-7e59-4526-8f86-e47f74910634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary : X-rays are a type of radiation called electromagnetic waves. X-ray imaging creates pictures of the inside of your body. The images show the parts of your body in different shades of black and white. This is because different tissues absorb different amounts of radiation. Calcium in bones absorbs x-rays the most, so bones look white. Fat and other soft tissues absorb less, and look gray. Air absorbs the least, so lungs look black. The most familiar use of x-rays is checking for broken bones, but x-rays are also used in other ways. For example, chest x-rays can spot pneumonia. Mammograms use x-rays to look for breast cancer. When you have an x-ray, you may wear a lead apron to protect certain parts of your body. The amount of radiation you get from an x-ray is small. For example, a chest x-ray gives out a radiation dose similar to the amount of radiation you're naturally exposed to from the environment over 10 days.\n"
     ]
    }
   ],
   "source": [
    "print(rag_results_df.iloc[0, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb6764f-7399-4d23-bbdb-8027c89aefb2",
   "metadata": {},
   "source": [
    "## RAGAS eval (unchanged API; uses ground_truth from the rows we built)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8bb63e6-dcb9-4a95-8128-5681e7c2f2ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys, subprocess\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-U\", \"langchain-huggingface\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f72039c-2513-4740-b2aa-d30f71b828c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anish\\anaconda3\\Lib\\site-packages\\ragas\\metrics\\__init__.py:1: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  from ragas.metrics._answer_correctness import AnswerCorrectness, answer_correctness\n",
      "C:\\Users\\anish\\anaconda3\\Lib\\site-packages\\ragas\\metrics\\__init__.py:4: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  from ragas.metrics._context_entities_recall import (\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc0d669931474362a19d689f406fac5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6d6d87f5a494bc095dce25b7da12f67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4c0833cee0c4288b01737f9744391e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9831d3fa7dab40b1b55e9ccc1030ab63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92bee77f129143e1a832aebf6c0ba275",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RAGAS METRICS (averages) ===\n",
      "faithfulness: 0.894\n",
      "answer_relevancy: 0.565\n",
      "context_precision: 1.000\n",
      "context_recall: 0.982\n",
      "answer_correctness: 0.697\n",
      "hallucination (1 - faithfulness): 0.106\n"
     ]
    }
   ],
   "source": [
    "# --- SAFER RAGAS EVAL with FastEmbed (sequential, retries, robust) ---\n",
    "\n",
    "# (Optional) tame threading to avoid native-lib crashes on Windows/Anaconda\n",
    "import os, time, math\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"1\"\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "\n",
    "# --- metrics (answer_correctness may not exist on older ragas) ---\n",
    "try:\n",
    "    from ragas.metrics import (\n",
    "        faithfulness,\n",
    "        answer_relevancy,\n",
    "        context_precision,\n",
    "        context_recall,\n",
    "        answer_correctness,\n",
    "    )\n",
    "    HAVE_CORR = True\n",
    "except Exception:\n",
    "    from ragas.metrics import (\n",
    "        faithfulness,\n",
    "        answer_relevancy,\n",
    "        context_precision,\n",
    "        context_recall,\n",
    "    )\n",
    "    HAVE_CORR = False\n",
    "\n",
    "# --- LLM judge via Groq (keep it cheap/fast) ---\n",
    "from langchain_groq import ChatGroq\n",
    "ragas_llm = ChatGroq(\n",
    "    groq_api_key=GROQ_API_KEY,\n",
    "    model_name=\"llama-3.1-8b-instant\",\n",
    "    temperature=0.0,\n",
    "    max_retries=6,          # help with transient errors\n",
    "    request_timeout=60,     # avoid hanging\n",
    ")\n",
    "\n",
    "# --- Embeddings: FastEmbed (fallback to TF-IDF if not available) ---\n",
    "def make_embeddings():\n",
    "    try:\n",
    "        from langchain_community.embeddings.fastembed import FastEmbedEmbeddings\n",
    "        return FastEmbedEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "    except Exception as e:\n",
    "        print(\"[WARN] FastEmbed unavailable -> falling back to TF-IDF:\", e)\n",
    "        # Minimal TF-IDF wrapper that satisfies LangChain Embeddings\n",
    "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "        from langchain_core.embeddings import Embeddings\n",
    "        class TfidfEmbeddings(Embeddings):\n",
    "            def __init__(self, corpus, max_features=4096):\n",
    "                self.vec = TfidfVectorizer(max_features=max_features)\n",
    "                self.vec.fit(corpus)\n",
    "            def embed_documents(self, texts):\n",
    "                return self.vec.transform(texts).toarray().tolist()\n",
    "            def embed_query(self, text):\n",
    "                return self.vec.transform([text]).toarray()[0].tolist()\n",
    "        # small fit corpus from your current batch\n",
    "        _fit = []\n",
    "        _fit += [t for ctxs in rag_results_df[\"contexts\"] for t in (ctxs if isinstance(ctxs, list) else [str(ctxs)])]\n",
    "        _fit += rag_results_df[\"answer\"].astype(str).tolist()\n",
    "        _fit += rag_results_df[\"ground_truth\"].astype(str).tolist()\n",
    "        _fit = [s for s in {str(x).strip() for x in _fit} if s]\n",
    "        return TfidfEmbeddings(_fit, max_features=4096)\n",
    "\n",
    "ragas_embeddings = make_embeddings()\n",
    "\n",
    "# --- Sanitize the dataframe to avoid bad rows ---\n",
    "def _as_list_str(x):\n",
    "    if isinstance(x, list):\n",
    "        return [str(t) for t in x if str(t).strip()]\n",
    "    if pd.isna(x) or str(x).strip() == \"\":\n",
    "        return []\n",
    "    return [str(x)]\n",
    "\n",
    "_df = rag_results_df.copy()\n",
    "_df[\"question\"]      = _df[\"question\"].astype(str).fillna(\"\").str.strip()\n",
    "_df[\"answer\"]        = _df[\"answer\"].astype(str).fillna(\"\").str.strip()\n",
    "_df[\"ground_truth\"]  = _df[\"ground_truth\"].astype(str).fillna(\"\").str.strip()\n",
    "_df[\"contexts\"]      = _df[\"contexts\"].apply(_as_list_str)\n",
    "\n",
    "# drop rows with empty question/answer/contexts or ground_truth (they break some metrics)\n",
    "mask_ok = _df[\"question\"].ne(\"\") & _df[\"answer\"].ne(\"\") & _df[\"ground_truth\"].ne(\"\") & _df[\"contexts\"].apply(len).gt(0)\n",
    "bad = (~mask_ok).sum()\n",
    "if bad:\n",
    "    print(f\"[INFO] Dropping {bad} invalid rows before RAGAS.\")\n",
    "_df = _df[mask_ok].reset_index(drop=True)\n",
    "\n",
    "# small cap to avoid 429s; tune as needed\n",
    "MAX_ROWS = None  # set e.g. 20 if you still hit rate limits\n",
    "if MAX_ROWS is not None:\n",
    "    _df = _df.head(MAX_ROWS).copy()\n",
    "\n",
    "ragas_ds = Dataset.from_pandas(_df[[\"question\",\"answer\",\"contexts\",\"ground_truth\"]])\n",
    "\n",
    "# --- Evaluate metrics one-by-one with polite retries (helps with Groq 429) ---\n",
    "def eval_metric_with_backoff(metric, tries=5, wait_seconds=70):\n",
    "    last_err = None\n",
    "    for t in range(1, tries+1):\n",
    "        try:\n",
    "            rep = evaluate(\n",
    "                dataset=ragas_ds,\n",
    "                metrics=[metric],\n",
    "                llm=ragas_llm,\n",
    "                embeddings=ragas_embeddings,\n",
    "                is_async=False,          # avoid heavy async executor\n",
    "                raise_exceptions=False,  # don’t crash on inner errors\n",
    "            )\n",
    "            return rep\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            print(f\"[RAGAS] Retry {t}/{tries} after {wait_seconds}s due to: {e}\")\n",
    "            time.sleep(wait_seconds)\n",
    "    raise RuntimeError(f\"RAGAS metric failed after {tries} retries\") from last_err\n",
    "\n",
    "metrics = [faithfulness, answer_relevancy, context_precision, context_recall]\n",
    "if HAVE_CORR:\n",
    "    metrics.append(answer_correctness)\n",
    "\n",
    "scores = {}\n",
    "for m in metrics:\n",
    "    rep = eval_metric_with_backoff(m)\n",
    "    key = next(iter(rep.keys()))\n",
    "    try:\n",
    "        scores[key] = float(rep[key])\n",
    "    except Exception:\n",
    "        # some versions return non-floats; coerce if possible\n",
    "        try:\n",
    "            scores[key] = float(str(rep[key]))\n",
    "        except Exception:\n",
    "            scores[key] = rep[key]\n",
    "\n",
    "print(\"\\n=== RAGAS METRICS (averages) ===\")\n",
    "order = [\"faithfulness\", \"answer_relevancy\", \"context_precision\", \"context_recall\"]\n",
    "if HAVE_CORR:\n",
    "    order.append(\"answer_correctness\")\n",
    "for k in order:\n",
    "    if k in scores:\n",
    "        v = scores[k]\n",
    "        print(f\"{k}: {v:.3f}\" if isinstance(v, float) else f\"{k}: {v}\")\n",
    "\n",
    "# Derived hallucination = 1 - faithfulness\n",
    "if \"faithfulness\" in scores and isinstance(scores[\"faithfulness\"], float):\n",
    "    print(f\"hallucination (1 - faithfulness): {1.0 - scores['faithfulness']:.3f}\")\n",
    "else:\n",
    "    print(\"hallucination (1 - faithfulness): N/A\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3eaaef3-5488-48c0-95dc-2dc416f91817",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190c71c5-9205-4e22-b2f7-7c28b702537b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
