{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "caa19d4c-3f92-4ebe-8a77-888fe0012fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\anish\\anaconda3\\lib\\site-packages (23.3.1)\n",
      "Collecting pip\n",
      "  Using cached pip-25.2-py3-none-any.whl.metadata (4.7 kB)\n",
      "Using cached pip-25.2-py3-none-any.whl (1.8 MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: To modify pip, please run the following command:\n",
      "C:\\Users\\anish\\anaconda3\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\anish\\anaconda3\\lib\\site-packages (2.1.4)\n",
      "Requirement already satisfied: faiss-cpu in c:\\users\\anish\\anaconda3\\lib\\site-packages (1.12.0)\n",
      "Requirement already satisfied: rank-bm25 in c:\\users\\anish\\anaconda3\\lib\\site-packages (0.2.2)\n",
      "Requirement already satisfied: fastembed in c:\\users\\anish\\anaconda3\\lib\\site-packages (0.7.3)\n",
      "Requirement already satisfied: langchain-community in c:\\users\\anish\\anaconda3\\lib\\site-packages (0.3.27)\n",
      "Requirement already satisfied: groq in c:\\users\\anish\\anaconda3\\lib\\site-packages (0.31.0)\n",
      "Requirement already satisfied: datasets in c:\\users\\anish\\anaconda3\\lib\\site-packages (2.20.0)\n",
      "Requirement already satisfied: ragas in c:\\users\\anish\\anaconda3\\lib\\site-packages (0.1.9)\n",
      "Requirement already satisfied: langchain-groq in c:\\users\\anish\\anaconda3\\lib\\site-packages (0.3.7)\n",
      "Requirement already satisfied: numpy<2,>=1.23.2 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\anish\\anaconda3\\lib\\site-packages (from faiss-cpu) (25.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.20 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from fastembed) (0.34.3)\n",
      "Requirement already satisfied: loguru<0.8.0,>=0.7.2 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from fastembed) (0.7.3)\n",
      "Requirement already satisfied: mmh3<6.0.0,>=4.1.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from fastembed) (5.2.0)\n",
      "Requirement already satisfied: onnxruntime!=1.20.0,>=1.17.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from fastembed) (1.22.1)\n",
      "Requirement already satisfied: pillow<12.0.0,>=10.3.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from fastembed) (11.3.0)\n",
      "Requirement already satisfied: py-rust-stemmers<0.2.0,>=0.1.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from fastembed) (0.1.5)\n",
      "Requirement already satisfied: requests<3.0,>=2.31 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from fastembed) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<1.0,>=0.15 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from fastembed) (0.19.1)\n",
      "Requirement already satisfied: tqdm<5.0,>=4.66 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from fastembed) (4.66.5)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from langchain-community) (0.3.74)\n",
      "Requirement already satisfied: langchain<1.0.0,>=0.3.26 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from langchain-community) (0.3.27)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from langchain-community) (2.0.42)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from langchain-community) (6.0.1)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from langchain-community) (3.9.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from langchain-community) (9.1.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from langchain-community) (2.10.1)\n",
      "Requirement already satisfied: langsmith>=0.1.125 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from langchain-community) (0.4.13)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from langchain-community) (0.4.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from groq) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from groq) (1.8.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from groq) (0.27.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from groq) (2.8.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\anish\\anaconda3\\lib\\site-packages (from groq) (1.3.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from groq) (4.12.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\anish\\anaconda3\\lib\\site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in c:\\users\\anish\\anaconda3\\lib\\site-packages (from datasets) (0.7)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: xxhash in c:\\users\\anish\\anaconda3\\lib\\site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\anish\\anaconda3\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets) (2023.10.0)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\anish\\anaconda3\\lib\\site-packages (from ragas) (0.9.0)\n",
      "Requirement already satisfied: langchain-openai in c:\\users\\anish\\anaconda3\\lib\\site-packages (from ragas) (0.3.29)\n",
      "Requirement already satisfied: openai>1 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from ragas) (1.99.7)\n",
      "Requirement already satisfied: pysbd>=0.3.4 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from ragas) (0.3.4)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\anish\\anaconda3\\lib\\site-packages (from ragas) (1.6.0)\n",
      "Requirement already satisfied: appdirs in c:\\users\\anish\\anaconda3\\lib\\site-packages (from ragas) (1.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.9.3)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from anyio<5,>=3.5.0->groq) (3.4)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.22.0)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\anish\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->groq) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\anish\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->groq) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from langchain<1.0.0,>=0.3.26->langchain-community) (0.3.9)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (1.33)\n",
      "Requirement already satisfied: orjson>=3.9.14 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from langsmith>=0.1.125->langchain-community) (3.10.7)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from langsmith>=0.1.125->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from langsmith>=0.1.125->langchain-community) (0.23.0)\n",
      "Requirement already satisfied: colorama>=0.3.4 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from loguru<0.8.0,>=0.7.2->fastembed) (0.4.6)\n",
      "Requirement already satisfied: win32-setctime>=1.0.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from loguru<0.8.0,>=0.7.2->fastembed) (1.2.0)\n",
      "Requirement already satisfied: coloredlogs in c:\\users\\anish\\anaconda3\\lib\\site-packages (from onnxruntime!=1.20.0,>=1.17.0->fastembed) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\anish\\anaconda3\\lib\\site-packages (from onnxruntime!=1.20.0,>=1.17.0->fastembed) (24.3.25)\n",
      "Requirement already satisfied: protobuf in c:\\users\\anish\\anaconda3\\lib\\site-packages (from onnxruntime!=1.20.0,>=1.17.0->fastembed) (4.25.8)\n",
      "Requirement already satisfied: sympy in c:\\users\\anish\\anaconda3\\lib\\site-packages (from onnxruntime!=1.20.0,>=1.17.0->fastembed) (1.13.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from openai>1->ragas) (0.9.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (2.20.1)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (0.4.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from requests<3.0,>=2.31->fastembed) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from requests<3.0,>=2.31->fastembed) (2.0.7)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.0.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from tiktoken->ragas) (2023.10.3)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain-community) (2.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from coloredlogs->onnxruntime!=1.20.0,>=1.17.0->fastembed) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from sympy->onnxruntime!=1.20.0,>=1.17.0->fastembed) (1.3.0)\n",
      "Requirement already satisfied: pyreadline3 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime!=1.20.0,>=1.17.0->fastembed) (3.5.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install pandas faiss-cpu rank-bm25 fastembed langchain-community groq datasets ragas langchain-groq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c53b1d2a-dfe3-4193-a864-0bf480c224fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Keep tokenizers & BLAS quiet and single-threaded to avoid kernel crashes\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c933687-c1c9-4a72-8d51-24c023db996a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### config (hard-coded to your saved artifacts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dedb08f7-7307-4e60-86bf-a3f6255ae077",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# ====== YOUR SAVED ARTIFACTS ======\n",
    "FAISS_INDEX_PATH = Path(\"faiss_medqa_bge_small.index\")   # FAISS index\n",
    "MAPPING_PATH     = Path(\"medqa_chunk_mapping.parquet\")   # id/text/meta (same order as FAISS)\n",
    "\n",
    "# ====== Retrieval must match the build ======\n",
    "EMBED_MODEL = \"BAAI/bge-small-en-v1.5\"   # you built with BGE-small\n",
    "USE_COSINE_IP = True                     # BGE + normalized vectors + IndexFlatIP\n",
    "USE_BGE_QUERY_INSTRUCTION = True\n",
    "BGE_QUERY_PREFIX = \"Represent this sentence for searching relevant passages: \"\n",
    "\n",
    "# Hybrid settings (pools + fusion)\n",
    "BM25_POOL_K   = 100     # how many BM25 candidates to consider before fusion\n",
    "FAISS_POOL_K  = 100     # how many FAISS candidates to consider before fusion\n",
    "RRF_K         = 60      # RRF constant (higher = flatter)\n",
    "FINAL_TOP_K   = 3       # how many contexts to send to the LLM\n",
    "\n",
    "# Questions (same as your other runs)\n",
    "EVAL_SELECTION_CSV = \"medquad_selected_questions.csv\"\n",
    "ANSWER_FIELD       = \"answer\"     # if CSV has 'gold', we detect it below\n",
    "N_EVAL             = 3            # reduce if hitting rate limits\n",
    "\n",
    "# LLM (only this changes for experiments)\n",
    "from groq import Groq\n",
    "GROQ_API_KEY = \"\"\n",
    "GEN_MODEL    = \"llama-3.3-70b-versatile\"\n",
    "\n",
    "PRINT_PROMPTS = False\n",
    "\n",
    "# RAGAS (optional)\n",
    "RUN_RAGAS_EVAL   = True\n",
    "RAGAS_JUDGE_MODEL = \"llama-3.1-8b-instant\"\n",
    "RAGAS_EMBED_MODEL = EMBED_MODEL   # FastEmbed via LangChain\n",
    "\n",
    "# Outputs\n",
    "OUT_DIR = \".\"\n",
    "ANSWERS_CSV = f\"{OUT_DIR}/hybrid_rag_answers.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "814fda21-87ef-46a3-bfc7-babc72da5800",
   "metadata": {},
   "outputs": [],
   "source": [
    "### load questions (same CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80f8d0d5-70c1-48c8-99f5-5a221e7b3bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid RAG questions: 3\n",
      "              Do you have information about X-Rays\n",
      "What are the symptoms of Alpha-ketoglutarate de...\n",
      "What are the treatments for GLUT1 deficiency sy...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sel = pd.read_csv(EVAL_SELECTION_CSV)\n",
    "gold_col = \"gold\" if \"gold\" in sel.columns else (ANSWER_FIELD if ANSWER_FIELD in sel.columns else None)\n",
    "assert gold_col is not None, f\"Selection file must contain either 'gold' or '{ANSWER_FIELD}'\"\n",
    "\n",
    "eval_df = sel[[\"question\", gold_col]].copy()\n",
    "if isinstance(N_EVAL, int):\n",
    "    eval_df = eval_df.head(N_EVAL)\n",
    "\n",
    "print(f\"Hybrid RAG questions: {len(eval_df)}\")\n",
    "print(eval_df[\"question\"].to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "388a20ae-c5ee-4bed-a50d-820d3f863ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### load FAISS index + mapping parquet (no re-indexing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9d2d7ff-f53a-4538-9e96-77a14ebadab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS ntotal: 18559 | mapping rows: 18559\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import faiss, json\n",
    "\n",
    "assert FAISS_INDEX_PATH.exists(), f\"Missing {FAISS_INDEX_PATH}\"\n",
    "assert MAPPING_PATH.exists(), f\"Missing {MAPPING_PATH}\"\n",
    "\n",
    "index = faiss.read_index(str(FAISS_INDEX_PATH))\n",
    "try:\n",
    "    faiss.omp_set_num_threads(1)  # keep FAISS single-threaded to avoid kernel churn\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "m = pd.read_parquet(MAPPING_PATH)\n",
    "\n",
    "ID_CANDS    = [\"id\",\"chunk_id\",\"doc_id\"]\n",
    "TEXT_CANDS  = [\"text\",\"chunk_text\",\"content\",\"passage\",\"body\",\"doc\"]\n",
    "META_CANDS  = [\"meta\",\"metadata\"]\n",
    "\n",
    "id_col   = next((c for c in ID_CANDS if c in m.columns), None)\n",
    "text_col = next((c for c in TEXT_CANDS if c in m.columns), None)\n",
    "meta_col = next((c for c in META_CANDS if c in m.columns), None)\n",
    "assert id_col and text_col, f\"Mapping parquet must have id/text. Found: {list(m.columns)}\"\n",
    "\n",
    "chunk_ids   = m[id_col].astype(str).tolist()\n",
    "chunk_texts = m[text_col].astype(str).tolist()\n",
    "if meta_col:\n",
    "    try:\n",
    "        chunk_meta  = [json.loads(x) if isinstance(x, str) else x for x in m[meta_col].tolist()]\n",
    "    except Exception:\n",
    "        chunk_meta = m[meta_col].tolist()\n",
    "else:\n",
    "    chunk_meta = [{} for _ in range(len(chunk_ids))]\n",
    "\n",
    "print(\"FAISS ntotal:\", index.ntotal, \"| mapping rows:\", len(chunk_ids))\n",
    "assert index.ntotal == len(chunk_ids), \"Index/mapping size mismatch — they must be built together.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d77cb28-fe6a-4eff-b1e9-56591689f1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### build BM25 over the same chunk_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79bb9b83-93b5-49a0-8770-5fa00775189b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM25 built over 18559 docs (same corpus as FAISS).\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "def simple_tokenize(text: str):\n",
    "    # lowercased alphanum + apostrophes; tune if you wish\n",
    "    return re.findall(r\"[A-Za-z0-9']+\", (text or \"\").lower())\n",
    "\n",
    "tokenized_docs = [simple_tokenize(t) for t in chunk_texts]\n",
    "bm25 = BM25Okapi(tokenized_docs)\n",
    "print(f\"BM25 built over {len(tokenized_docs)} docs (same corpus as FAISS).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3cdd3046-7a60-4825-a468-6810c4733f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### FastEmbed query encoder (same embed model as build)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5f79ef8-b833-41e8-9ae1-1feac64d691f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from langchain_community.embeddings.fastembed import FastEmbedEmbeddings\n",
    "\n",
    "q_embedder = FastEmbedEmbeddings(model_name=EMBED_MODEL)\n",
    "\n",
    "def _format_query(q: str) -> str:\n",
    "    return (BGE_QUERY_PREFIX + q) if USE_BGE_QUERY_INSTRUCTION else q\n",
    "\n",
    "def dense_retrieve_faiss(query: str, k: int = FAISS_POOL_K):\n",
    "    qtext = _format_query(query)\n",
    "    q_vec = np.asarray(q_embedder.embed_query(qtext), dtype=\"float32\")[None, :]\n",
    "    if USE_COSINE_IP:\n",
    "        q_vec /= (np.linalg.norm(q_vec, axis=1, keepdims=True) + 1e-12)\n",
    "    sims, idxs = index.search(q_vec, k)\n",
    "    idxs = idxs[0].tolist(); sims = sims[0].tolist()\n",
    "    return [(i, float(sims[j])) for j, i in enumerate(idxs) if i >= 0]\n",
    "\n",
    "def sparse_retrieve_bm25(query: str, k: int = BM25_POOL_K):\n",
    "    q_tokens = simple_tokenize(query)\n",
    "    scores = bm25.get_scores(q_tokens)\n",
    "    top_idx = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:k]\n",
    "    return [(i, float(scores[i])) for i in top_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48de3f74-14d4-4fdc-8590-53b8586c91f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hybrid fusion (RRF) + prompt + Groq chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cedfa536-1128-4299-9356-ed142fbb3933",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "from groq import Groq\n",
    "\n",
    "client = Groq(api_key=GROQ_API_KEY)\n",
    "\n",
    "def rrf_fuse(sparse_list, dense_list, k_rrf=RRF_K, final_top_k=FINAL_TOP_K):\n",
    "    \"\"\"\n",
    "    sparse_list / dense_list: lists of (idx, score) sorted descending by their own score.\n",
    "    We ignore raw scores and fuse by reciprocal ranks.\n",
    "    \"\"\"\n",
    "    fused = {}\n",
    "    # assign ranks (1-based)\n",
    "    for rank, (i, _s) in enumerate(sparse_list, start=1):\n",
    "        fused[i] = fused.get(i, 0.0) + 1.0 / (k_rrf + rank)\n",
    "    for rank, (i, _s) in enumerate(dense_list, start=1):\n",
    "        fused[i] = fused.get(i, 0.0) + 1.0 / (k_rrf + rank)\n",
    "    # sort by fused score\n",
    "    ranked = sorted(fused.items(), key=lambda x: x[1], reverse=True)[:final_top_k]\n",
    "    return ranked  # list of (idx, fused_score)\n",
    "\n",
    "def build_rag_messages(question: str, contexts: List[str], print_prompt: bool = False) -> List[Dict]:\n",
    "    system_txt = (\n",
    "        \"You are a concise, evidence-focused medical assistant. \"\n",
    "        \"Use the provided context passages to answer accurately. \"\n",
    "        \"If the context does not contain the answer, say you don't know.\"\n",
    "    )\n",
    "    ctx_block = \"\\n\\n\".join([f\"[Context {i+1}]\\n{c}\" for i, c in enumerate(contexts)])\n",
    "    user_txt = (\n",
    "        f\"Question: {question}\\n\\n\"\n",
    "        f\"Context Passages:\\n{ctx_block}\\n\\n\"\n",
    "        \"Instructions: Answer in 2–4 sentences. Cite which Context numbers support your statements (e.g., [1], [2]). \"\n",
    "        \"If insufficient evidence, say 'I don't know based on the given context.'\"\n",
    "    )\n",
    "    msgs = [{\"role\":\"system\",\"content\":system_txt},{\"role\":\"user\",\"content\":user_txt}]\n",
    "    if print_prompt:\n",
    "        print(\"\\n\"+\"=\"*88); print(\"[RAG PROMPT]\")\n",
    "        print(\"\\n[SYSTEM]\\n\"+system_txt)\n",
    "        print(\"\\n[USER]\\n\"+user_txt)\n",
    "        print(\"=\"*88)\n",
    "    return msgs\n",
    "\n",
    "def groq_chat(messages, model=GEN_MODEL, temperature=0.0, max_tokens=512, top_p=1.0) -> str:\n",
    "    r = client.chat.completions.create(\n",
    "        model=model, temperature=temperature, max_tokens=max_tokens, top_p=top_p, messages=messages\n",
    "    )\n",
    "    return r.choices[0].message.content.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b342cb51-4adf-463e-a3b4-9c8835c6c9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Run HYBRID RAG; save answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "62817247-a42d-4200-b022-2edfc42e0f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved HYBRID answers to: ./hybrid_rag_answers.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>contexts</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>final_top_k</th>\n",
       "      <th>bm25_pool_k</th>\n",
       "      <th>faiss_pool_k</th>\n",
       "      <th>rrf_k</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Do you have information about X-Rays</td>\n",
       "      <td>X-rays are a type of electromagnetic radiation...</td>\n",
       "      <td>[Summary : X-rays are a type of radiation call...</td>\n",
       "      <td>Summary : X-rays are a type of radiation calle...</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are the symptoms of Alpha-ketoglutarate d...</td>\n",
       "      <td>The symptoms of Alpha-ketoglutarate dehydrogen...</td>\n",
       "      <td>[What are the signs and symptoms of Alpha-keto...</td>\n",
       "      <td>What are the signs and symptoms of Alpha-ketog...</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0               Do you have information about X-Rays   \n",
       "1  What are the symptoms of Alpha-ketoglutarate d...   \n",
       "\n",
       "                                              answer  \\\n",
       "0  X-rays are a type of electromagnetic radiation...   \n",
       "1  The symptoms of Alpha-ketoglutarate dehydrogen...   \n",
       "\n",
       "                                            contexts  \\\n",
       "0  [Summary : X-rays are a type of radiation call...   \n",
       "1  [What are the signs and symptoms of Alpha-keto...   \n",
       "\n",
       "                                        ground_truth  final_top_k  \\\n",
       "0  Summary : X-rays are a type of radiation calle...            3   \n",
       "1  What are the signs and symptoms of Alpha-ketog...            3   \n",
       "\n",
       "   bm25_pool_k  faiss_pool_k  rrf_k  \n",
       "0          100           100     60  \n",
       "1          100           100     60  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows = []\n",
    "for _, r in eval_df.iterrows():\n",
    "    q  = str(r[\"question\"]).strip()\n",
    "    gt = str(r[gold_col]).strip()\n",
    "\n",
    "    # retrieve pools\n",
    "    dense_pool  = dense_retrieve_faiss(q, k=FAISS_POOL_K)\n",
    "    sparse_pool = sparse_retrieve_bm25(q,  k=BM25_POOL_K)\n",
    "\n",
    "    # fuse\n",
    "    fused = rrf_fuse(sparse_pool, dense_pool, k_rrf=RRF_K, final_top_k=FINAL_TOP_K)\n",
    "    final_indices = [i for (i, _f) in fused]\n",
    "    contexts = [chunk_texts[i] for i in final_indices]\n",
    "\n",
    "    # prompt + generate\n",
    "    msgs = build_rag_messages(q, contexts, print_prompt=PRINT_PROMPTS)\n",
    "    ans  = groq_chat(msgs, model=GEN_MODEL, temperature=0.0, max_tokens=512, top_p=1.0)\n",
    "\n",
    "    rows.append({\n",
    "        \"question\": q,\n",
    "        \"answer\": ans,\n",
    "        \"contexts\": contexts,      # list[str] for RAGAS\n",
    "        \"ground_truth\": gt,        # RAGAS expects 'ground_truth'; this does NOT rename your CSV\n",
    "        \"final_top_k\": FINAL_TOP_K,\n",
    "        \"bm25_pool_k\": BM25_POOL_K,\n",
    "        \"faiss_pool_k\": FAISS_POOL_K,\n",
    "        \"rrf_k\": RRF_K\n",
    "    })\n",
    "\n",
    "hybrid_df = pd.DataFrame(rows)\n",
    "hybrid_df.to_csv(ANSWERS_CSV, index=False)\n",
    "print(f\"Saved HYBRID answers to: {ANSWERS_CSV}\")\n",
    "hybrid_df.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "418bccdf-ff2e-4747-b63b-56a159d1648d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### RAGAS evaluation (FastEmbed + ChatGroq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e33343ff-9892-43e3-a273-d92086158a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anish\\anaconda3\\Lib\\site-packages\\ragas\\metrics\\__init__.py:1: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  from ragas.metrics._answer_correctness import AnswerCorrectness, answer_correctness\n",
      "C:\\Users\\anish\\anaconda3\\Lib\\site-packages\\ragas\\metrics\\__init__.py:4: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  from ragas.metrics._context_entities_recall import (\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d120bd7e981547c1a59c9f2e0f839525",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1127b08efa974bc7a6e0ede578acbbed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58a28073762149beb01c2e17b7a36c4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f32c84e98da34850b04cbf254621cfe6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f716e0890b25422baf2433373779478a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RAGAS (Hybrid) ===\n",
      "faithfulness: 0.875\n",
      "answer_relevancy: 0.526\n",
      "context_precision: 1.000\n",
      "context_recall: 0.816\n",
      "answer_correctness: 0.584\n",
      "Saved: ./hybrid_ragas_metrics.json\n"
     ]
    }
   ],
   "source": [
    "if RUN_RAGAS_EVAL:\n",
    "    try:\n",
    "        from datasets import Dataset\n",
    "        from ragas import evaluate\n",
    "        try:\n",
    "            from ragas.metrics import faithfulness, answer_relevancy, context_precision, context_recall, answer_correctness\n",
    "            METRICS = [faithfulness, answer_relevancy, context_precision, context_recall, answer_correctness]\n",
    "        except Exception:\n",
    "            from ragas.metrics import faithfulness, answer_relevancy, context_precision, context_recall\n",
    "            METRICS = [faithfulness, answer_relevancy, context_precision, context_recall]\n",
    "\n",
    "        from langchain_groq import ChatGroq\n",
    "        from langchain_community.embeddings.fastembed import FastEmbedEmbeddings as LC_FastEmbedEmbeddings\n",
    "\n",
    "        ragas_llm = ChatGroq(\n",
    "            groq_api_key=GROQ_API_KEY, model_name=RAGAS_JUDGE_MODEL,\n",
    "            temperature=0.0, max_retries=6, request_timeout=60\n",
    "        )\n",
    "        ragas_embeddings = LC_FastEmbedEmbeddings(model_name=RAGAS_EMBED_MODEL)\n",
    "\n",
    "        ds = Dataset.from_pandas(hybrid_df[[\"question\",\"answer\",\"contexts\",\"ground_truth\"]].copy())\n",
    "\n",
    "        # run one metric at a time (more robust with rate limits)\n",
    "        scores = {}\n",
    "        for metric in METRICS:\n",
    "            for attempt in range(5):\n",
    "                try:\n",
    "                    rep = evaluate(dataset=ds, metrics=[metric], llm=ragas_llm, embeddings=ragas_embeddings,\n",
    "                                   is_async=False, raise_exceptions=False)\n",
    "                    key = next(iter(rep.keys()))\n",
    "                    try: scores[key] = float(rep[key])\n",
    "                    except: scores[key] = float(str(rep[key]))\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    import time\n",
    "                    print(f\"[RAGAS] retry {attempt+1}/5 due to: {e}\")\n",
    "                    time.sleep(30 + attempt*20)\n",
    "            else:\n",
    "                scores[str(metric)] = None\n",
    "\n",
    "        import json\n",
    "        out_json = f\"{OUT_DIR}/hybrid_ragas_metrics.json\"\n",
    "        with open(out_json, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(scores, f, indent=2)\n",
    "        print(\"\\n=== RAGAS (Hybrid) ===\")\n",
    "        for k, v in scores.items():\n",
    "            if isinstance(v, float):\n",
    "                print(f\"{k}: {v:.3f}\")\n",
    "            else:\n",
    "                print(f\"{k}: {v}\")\n",
    "        print(\"Saved:\", out_json)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"[RAGAS WARNING] Skipped due to error:\", e)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
