{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7c3fdbb-8d71-4258-860e-3e8d3a5837d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\anish\\anaconda3\\lib\\site-packages (23.3.1)\n",
      "Collecting pip\n",
      "  Using cached pip-25.2-py3-none-any.whl.metadata (4.7 kB)\n",
      "Using cached pip-25.2-py3-none-any.whl (1.8 MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: To modify pip, please run the following command:\n",
      "C:\\Users\\anish\\anaconda3\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faiss-cpu in c:\\users\\anish\\anaconda3\\lib\\site-packages (1.12.0)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\anish\\anaconda3\\lib\\site-packages (3.0.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\anish\\anaconda3\\lib\\site-packages (2.1.4)\n",
      "Requirement already satisfied: groq in c:\\users\\anish\\anaconda3\\lib\\site-packages (0.31.0)\n",
      "Requirement already satisfied: datasets in c:\\users\\anish\\anaconda3\\lib\\site-packages (2.20.0)\n",
      "Requirement already satisfied: ragas in c:\\users\\anish\\anaconda3\\lib\\site-packages (0.1.9)\n",
      "Requirement already satisfied: langchain_groq in c:\\users\\anish\\anaconda3\\lib\\site-packages (0.3.7)\n",
      "Requirement already satisfied: langchain_community in c:\\users\\anish\\anaconda3\\lib\\site-packages (0.3.27)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\anish\\anaconda3\\lib\\site-packages (from faiss-cpu) (25.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.44.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\anish\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.66.5)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from sentence-transformers) (2.5.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\anish\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\anish\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.11.4)\n",
      "Requirement already satisfied: huggingface-hub>=0.15.1 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.34.3)\n",
      "Requirement already satisfied: Pillow in c:\\users\\anish\\anaconda3\\lib\\site-packages (from sentence-transformers) (11.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from groq) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from groq) (1.8.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from groq) (0.27.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from groq) (2.8.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\anish\\anaconda3\\lib\\site-packages (from groq) (1.3.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from groq) (4.12.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\anish\\anaconda3\\lib\\site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in c:\\users\\anish\\anaconda3\\lib\\site-packages (from datasets) (0.7)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: xxhash in c:\\users\\anish\\anaconda3\\lib\\site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\anish\\anaconda3\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\anish\\anaconda3\\lib\\site-packages (from datasets) (3.9.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\anish\\anaconda3\\lib\\site-packages (from ragas) (0.9.0)\n",
      "Requirement already satisfied: langchain in c:\\users\\anish\\anaconda3\\lib\\site-packages (from ragas) (0.3.27)\n",
      "Requirement already satisfied: langchain-core in c:\\users\\anish\\anaconda3\\lib\\site-packages (from ragas) (0.3.74)\n",
      "Requirement already satisfied: langchain-openai in c:\\users\\anish\\anaconda3\\lib\\site-packages (from ragas) (0.3.29)\n",
      "Requirement already satisfied: openai>1 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from ragas) (1.99.7)\n",
      "Requirement already satisfied: pysbd>=0.3.4 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from ragas) (0.3.4)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\anish\\anaconda3\\lib\\site-packages (from ragas) (1.6.0)\n",
      "Requirement already satisfied: appdirs in c:\\users\\anish\\anaconda3\\lib\\site-packages (from ragas) (1.4.4)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from langchain_community) (2.0.42)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from langchain_community) (9.1.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from langchain_community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from langchain_community) (2.10.1)\n",
      "Requirement already satisfied: langsmith>=0.1.125 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from langchain_community) (0.4.13)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from langchain_community) (0.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.9.3)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from anyio<5,>=3.5.0->groq) (3.4)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.22.0)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\anish\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->groq) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\anish\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->groq) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from langchain->ragas) (0.3.9)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from langchain-core->ragas) (1.33)\n",
      "Requirement already satisfied: orjson>=3.9.14 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from langsmith>=0.1.125->langchain_community) (3.10.7)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from langsmith>=0.1.125->langchain_community) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from langsmith>=0.1.125->langchain_community) (0.23.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from openai>1->ragas) (0.9.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (2.20.1)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.1.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (0.4.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.0.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\anish\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\anish\\anaconda3\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2023.10.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.19.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (2.2.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core->ragas) (2.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\anish\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install faiss-cpu sentence-transformers pandas groq datasets ragas langchain_groq langchain_community\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "370928c8-69d5-41c0-896a-7cdcd3ac7e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Keep tokenizers & BLAS quiet and single-threaded to avoid kernel crashes\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ef29089-4e40-446e-be0c-59c27cfa5386",
   "metadata": {},
   "outputs": [],
   "source": [
    "### config (uses your saved files, FastEmbed, same questions CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18817a3b-c9c2-4dde-8e2f-3b9574c730ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Hard-coded artifacts you shared =====\n",
    "from pathlib import Path\n",
    "\n",
    "FAISS_INDEX_PATH = Path(\"faiss_medqa_bge_small.index\")   # your FAISS file\n",
    "MAPPING_PATH     = Path(\"medqa_chunk_mapping.parquet\")   # id/text/meta table (same order as index)\n",
    "\n",
    "# If you ever need the chunks parquet, keep it here (not required for this run):\n",
    "CHUNKS_PARQUET   = Path(\"medqa_chunks_token.parquet\")\n",
    "\n",
    "# ===== Retrieval must match how the index was built =====\n",
    "EMBED_MODEL = \"BAAI/bge-small-en-v1.5\"   # you used BGE-small; keep the same\n",
    "USE_COSINE_IP = True                     # BGE is typically normalized + IndexFlatIP\n",
    "USE_BGE_QUERY_INSTRUCTION = True         # set False if you did NOT use this during build\n",
    "BGE_QUERY_PREFIX = \"Represent this sentence for searching relevant passages: \"\n",
    "\n",
    "TOP_K_LIST = [3]                          # keep simple; you can add [5,10] later\n",
    "\n",
    "# ===== SAME questions CSV you use for prompting =====\n",
    "EVAL_SELECTION_CSV = \"medquad_selected_questions.csv\"\n",
    "ANSWER_FIELD       = \"answer\"             # if your CSV has 'gold', we auto-detect below\n",
    "N_EVAL             = 3                    # reduce if you hit rate limits\n",
    "\n",
    "# ===== Groq LLM (only this changes for your experiment) =====\n",
    "from groq import Groq\n",
    "GROQ_API_KEY = \"\"\n",
    "GEN_MODEL    = \"llama-3.3-70b-versatile\"     # <— just change this to test another Groq LLM\n",
    "\n",
    "PRINT_PROMPTS = False\n",
    "\n",
    "# ===== RAGAS (can switch off to avoid rate limits) =====\n",
    "RUN_RAGAS_EVAL   = True\n",
    "RAGAS_JUDGE_MODEL = \"llama-3.1-8b-instant\"\n",
    "RAGAS_EMBED_MODEL = EMBED_MODEL           # FastEmbed in LangChain\n",
    "\n",
    "# ===== Output =====\n",
    "OUT_DIR = \".\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf514fd1-251b-4c3d-96ff-35887bd117a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### keep the kernel light (threads + env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73db1b08-4fa9-4b2b-80e1-8bc354d21140",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6df789b7-4d09-47da-9083-f5e05294b1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "### load questions (same CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c251668-ffc6-4a83-980d-3c96a6f83668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval questions: 3\n",
      "              Do you have information about X-Rays\n",
      "What are the symptoms of Alpha-ketoglutarate de...\n",
      "What are the treatments for GLUT1 deficiency sy...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sel = pd.read_csv(EVAL_SELECTION_CSV)\n",
    "\n",
    "gold_col = \"gold\" if \"gold\" in sel.columns else (ANSWER_FIELD if ANSWER_FIELD in sel.columns else None)\n",
    "assert gold_col is not None, f\"Selection file must contain either 'gold' or '{ANSWER_FIELD}'\"\n",
    "\n",
    "eval_df = sel[[\"question\", gold_col]].copy()\n",
    "if isinstance(N_EVAL, int):\n",
    "    eval_df = eval_df.head(N_EVAL)\n",
    "\n",
    "print(f\"Eval questions: {len(eval_df)}\")\n",
    "print(eval_df['question'].to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56f3dbe8-5f5e-4c21-82d1-d53eca8fcc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### load FAISS + mapping (no reindexing, no re-chunking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43e479b7-bce6-4bf0-af0a-84a99aff4cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS ntotal: 18559 | mapping rows: 18559\n"
     ]
    }
   ],
   "source": [
    "import faiss, json\n",
    "\n",
    "assert FAISS_INDEX_PATH.exists(), f\"Missing {FAISS_INDEX_PATH}\"\n",
    "assert MAPPING_PATH.exists(), f\"Missing {MAPPING_PATH}\"\n",
    "\n",
    "index = faiss.read_index(str(FAISS_INDEX_PATH))\n",
    "try:\n",
    "    faiss.omp_set_num_threads(1)  # keep FAISS single-threaded — fewer crashes\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "m = pd.read_parquet(MAPPING_PATH)\n",
    "\n",
    "# Accept either {id,text,meta} or similar column names:\n",
    "ID_COL_CAND    = [\"id\",\"chunk_id\",\"doc_id\"]\n",
    "TEXT_COL_CAND  = [\"text\",\"chunk_text\",\"content\",\"passage\",\"body\",\"doc\"]\n",
    "META_COL_CAND  = [\"meta\",\"metadata\"]\n",
    "\n",
    "id_col   = next((c for c in ID_COL_CAND if c in m.columns), None)\n",
    "text_col = next((c for c in TEXT_COL_CAND if c in m.columns), None)\n",
    "meta_col = next((c for c in META_COL_CAND if c in m.columns), None)\n",
    "\n",
    "assert id_col and text_col, f\"Mapping parquet must have id/text. Found: {list(m.columns)}\"\n",
    "\n",
    "chunk_ids   = m[id_col].astype(str).tolist()\n",
    "chunk_texts = m[text_col].astype(str).tolist()\n",
    "if meta_col:\n",
    "    try:\n",
    "        chunk_meta  = [json.loads(x) if isinstance(x, str) else x for x in m[meta_col].tolist()]\n",
    "    except Exception:\n",
    "        chunk_meta = m[meta_col].tolist()\n",
    "else:\n",
    "    chunk_meta = [{} for _ in range(len(chunk_ids))]\n",
    "\n",
    "print(\"FAISS ntotal:\", index.ntotal, \"| mapping rows:\", len(chunk_ids))\n",
    "assert index.ntotal == len(chunk_ids), \"Index/mapping size mismatch — they must be built together.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3737631-aad1-4808-84f6-f7632d031207",
   "metadata": {},
   "outputs": [],
   "source": [
    "### FastEmbed (same as your build) + retrieval helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2ffe601-9bdf-46e7-9962-6588383354cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from langchain_community.embeddings.fastembed import FastEmbedEmbeddings\n",
    "\n",
    "# Light-weight CPU embedder (downloads model once; much lighter than sentence-transformers)\n",
    "q_embedder = FastEmbedEmbeddings(model_name=EMBED_MODEL)\n",
    "\n",
    "def _format_query(q: str) -> str:\n",
    "    if USE_BGE_QUERY_INSTRUCTION:\n",
    "        return BGE_QUERY_PREFIX + q\n",
    "    return q\n",
    "\n",
    "def retrieve_topk_faiss(query: str, k: int):\n",
    "    qtext = _format_query(query)\n",
    "    # embed_query returns a single vector\n",
    "    q_vec = np.asarray(q_embedder.embed_query(qtext), dtype=\"float32\")[None, :]\n",
    "    if USE_COSINE_IP:\n",
    "        # Normalize for cosine via inner product\n",
    "        q_vec /= (np.linalg.norm(q_vec, axis=1, keepdims=True) + 1e-12)\n",
    "    sims, idxs = index.search(q_vec, k)   # (1,k)\n",
    "    idxs = idxs[0].tolist(); sims = sims[0].tolist()\n",
    "    hits = []\n",
    "    for rank, (i, s) in enumerate(zip(idxs, sims), start=1):\n",
    "        if i < 0: continue\n",
    "        hits.append((chunk_ids[i], chunk_texts[i], float(s)))\n",
    "    return hits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44cabb6c-4e58-494e-97c3-bb0ab6a060bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### RAG prompt + Groq chat (same template you used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "06e71957-4f6f-4c08-8f26-e092e1d32fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "from groq import Groq\n",
    "\n",
    "client_groq = Groq(api_key=GROQ_API_KEY)\n",
    "\n",
    "def build_rag_messages(question: str, contexts: List[str], print_prompt: bool = False) -> List[Dict]:\n",
    "    system_txt = (\n",
    "        \"You are a concise, evidence-focused medical assistant. \"\n",
    "        \"Use the provided context passages to answer accurately. \"\n",
    "        \"If the context does not contain the answer, say you don't know.\"\n",
    "    )\n",
    "    ctx_block = \"\\n\\n\".join([f\"[Context {i+1}]\\n{c}\" for i, c in enumerate(contexts)])\n",
    "    user_txt = (\n",
    "        f\"Question: {question}\\n\\n\"\n",
    "        f\"Context Passages:\\n{ctx_block}\\n\\n\"\n",
    "        \"Instructions: Answer in 2–4 sentences. Cite which Context numbers support your statements (e.g., [1], [2]). \"\n",
    "        \"If insufficient evidence, say 'I don't know based on the given context.'\"\n",
    "    )\n",
    "    msgs = [{\"role\":\"system\",\"content\":system_txt},{\"role\":\"user\",\"content\":user_txt}]\n",
    "    if print_prompt:\n",
    "        print(\"\\n\"+\"=\"*88); print(\"[RAG PROMPT]\")\n",
    "        print(\"\\n[SYSTEM]\\n\"+system_txt)\n",
    "        print(\"\\n[USER]\\n\"+user_txt)\n",
    "        print(\"=\"*88)\n",
    "    return msgs\n",
    "\n",
    "def groq_chat(messages, model=GEN_MODEL, temperature=0.0, max_tokens=512, top_p=1.0) -> str:\n",
    "    r = client_groq.chat.completions.create(\n",
    "        model=model, temperature=temperature, max_tokens=max_tokens, top_p=top_p, messages=messages\n",
    "    )\n",
    "    return r.choices[0].message.content.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "64ee79e8-f9d7-46f5-96aa-8e742e635174",
   "metadata": {},
   "outputs": [],
   "source": [
    "### run Dense RAG (reuse index; no heavy memory); save answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b60e85b6-4bf5-4d9e-8801-cb0777458c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================================================================\n",
      "[RUN] Dense RAG with TOP_K=3  |  GEN_MODEL=llama-3.3-70b-versatile\n",
      "========================================================================================\n",
      "[RUN] Saved answers: ./dense_rag_k3_answers.csv  (rows=3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>contexts</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>top_k</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Do you have information about X-Rays</td>\n",
       "      <td>X-rays are a type of radiation called electrom...</td>\n",
       "      <td>[ signs of disease. X ray. An x ray is a pictu...</td>\n",
       "      <td>Summary : X-rays are a type of radiation calle...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are the symptoms of Alpha-ketoglutarate d...</td>\n",
       "      <td>The symptoms of Alpha-ketoglutarate dehydrogen...</td>\n",
       "      <td>[What are the signs and symptoms of Alpha-keto...</td>\n",
       "      <td>What are the signs and symptoms of Alpha-ketog...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0               Do you have information about X-Rays   \n",
       "1  What are the symptoms of Alpha-ketoglutarate d...   \n",
       "\n",
       "                                              answer  \\\n",
       "0  X-rays are a type of radiation called electrom...   \n",
       "1  The symptoms of Alpha-ketoglutarate dehydrogen...   \n",
       "\n",
       "                                            contexts  \\\n",
       "0  [ signs of disease. X ray. An x ray is a pictu...   \n",
       "1  [What are the signs and symptoms of Alpha-keto...   \n",
       "\n",
       "                                        ground_truth  top_k  \n",
       "0  Summary : X-rays are a type of radiation calle...      3  \n",
       "1  What are the signs and symptoms of Alpha-ketog...      3  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows_all = []\n",
    "\n",
    "for K in TOP_K_LIST:\n",
    "    print(\"\\n\" + \"=\"*88)\n",
    "    print(f\"[RUN] Dense RAG with TOP_K={K}  |  GEN_MODEL={GEN_MODEL}\")\n",
    "    print(\"=\"*88)\n",
    "\n",
    "    rows = []\n",
    "    for _, r in eval_df.iterrows():\n",
    "        q  = str(r[\"question\"]).strip()\n",
    "        gt = str(r[gold_col]).strip()\n",
    "\n",
    "        hits = retrieve_topk_faiss(q, k=K)\n",
    "        contexts = [txt for (_id, txt, _s) in hits]\n",
    "        msgs = build_rag_messages(q, contexts, print_prompt=PRINT_PROMPTS)\n",
    "        ans = groq_chat(msgs, model=GEN_MODEL, temperature=0.0, max_tokens=512, top_p=1.0)\n",
    "\n",
    "        rows.append({\"question\": q, \"answer\": ans, \"contexts\": contexts, \"ground_truth\": gt, \"top_k\": K})\n",
    "\n",
    "    run_df = pd.DataFrame(rows)\n",
    "    out_csv = f\"{OUT_DIR}/dense_rag_k{K}_answers.csv\"\n",
    "    run_df.to_csv(out_csv, index=False)\n",
    "    print(f\"[RUN] Saved answers: {out_csv}  (rows={len(run_df)})\")\n",
    "    rows_all.append(run_df)\n",
    "\n",
    "rag_results_df = pd.concat(rows_all, ignore_index=True)\n",
    "rag_results_df.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10bccf3e-bf53-455c-95f9-25d09f41d7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### RAGAS with FastEmbed (robust, one metric at a time w/ retry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2189166b-15ef-4685-9eb2-5c465bd75472",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anish\\anaconda3\\Lib\\site-packages\\ragas\\metrics\\__init__.py:1: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  from ragas.metrics._answer_correctness import AnswerCorrectness, answer_correctness\n",
      "C:\\Users\\anish\\anaconda3\\Lib\\site-packages\\ragas\\metrics\\__init__.py:4: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  from ragas.metrics._context_entities_recall import (\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bf53ec9f5d94dc5bd1b3c4ca45fa132",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b42c5bc82b294fe9a2d7d1d3dd6171c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "793a8eb0a53f41a7a9af765a3d5cdff7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "284b463998c249a9b4c0403aa866a24b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cd4287953fc4cf78b3fbbb697fd4d30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RAGAS SUMMARY (Dense RAG) ===\n",
      " TOP_K  faithfulness  answer_relevancy  context_precision  context_recall  answer_correctness\n",
      "     3      0.916667          0.901494                1.0        0.608974             0.69494\n",
      "Saved: ./dense_rag_ragas_summary.csv\n"
     ]
    }
   ],
   "source": [
    "if RUN_RAGAS_EVAL:\n",
    "    try:\n",
    "        from datasets import Dataset\n",
    "        from ragas import evaluate\n",
    "        # try to import answer_correctness if available\n",
    "        try:\n",
    "            from ragas.metrics import faithfulness, answer_relevancy, context_precision, context_recall, answer_correctness\n",
    "            METRICS = [faithfulness, answer_relevancy, context_precision, context_recall, answer_correctness]\n",
    "        except Exception:\n",
    "            from ragas.metrics import faithfulness, answer_relevancy, context_precision, context_recall\n",
    "            METRICS = [faithfulness, answer_relevancy, context_precision, context_recall]\n",
    "\n",
    "        from langchain_groq import ChatGroq\n",
    "        from langchain_community.embeddings.fastembed import FastEmbedEmbeddings as LC_FastEmbedEmbeddings\n",
    "\n",
    "        ragas_llm = ChatGroq(\n",
    "            groq_api_key=GROQ_API_KEY, model_name=RAGAS_JUDGE_MODEL,\n",
    "            temperature=0.0, max_retries=6, request_timeout=60\n",
    "        )\n",
    "        ragas_embeddings = LC_FastEmbedEmbeddings(model_name=RAGAS_EMBED_MODEL)\n",
    "\n",
    "        # evaluate per K to keep loads small and avoid 429s\n",
    "        ablation_rows = []\n",
    "        for K in sorted(rag_results_df[\"top_k\"].unique()):\n",
    "            sub = rag_results_df[rag_results_df[\"top_k\"] == K][[\"question\",\"answer\",\"contexts\",\"ground_truth\"]].copy()\n",
    "            ds  = Dataset.from_pandas(sub)\n",
    "\n",
    "            scores = {}\n",
    "            for m in METRICS:\n",
    "                for attempt in range(5):\n",
    "                    try:\n",
    "                        rep = evaluate(dataset=ds, metrics=[m], llm=ragas_llm, embeddings=ragas_embeddings,\n",
    "                                       is_async=False, raise_exceptions=False)\n",
    "                        key = next(iter(rep.keys()))\n",
    "                        try: scores[key] = float(rep[key])\n",
    "                        except: scores[key] = float(str(rep[key]))\n",
    "                        break\n",
    "                    except Exception as e:\n",
    "                        import time\n",
    "                        print(f\"[RAGAS][K={K}] retry {attempt+1}/5 due to: {e}\")\n",
    "                        time.sleep(30 + attempt*20)\n",
    "                else:\n",
    "                    scores[str(m)] = None\n",
    "\n",
    "            row = {\"TOP_K\": int(K)}\n",
    "            for k in [\"faithfulness\",\"answer_relevancy\",\"context_precision\",\"context_recall\",\"answer_correctness\"]:\n",
    "                if k in scores and isinstance(scores[k], float):\n",
    "                    row[k] = scores[k]\n",
    "            ablation_rows.append(row)\n",
    "\n",
    "        ablate_df = pd.DataFrame(ablation_rows).sort_values(by=\"faithfulness\", ascending=False)\n",
    "        out_summary = f\"{OUT_DIR}/dense_rag_ragas_summary.csv\"\n",
    "        ablate_df.to_csv(out_summary, index=False)\n",
    "        print(\"\\n=== RAGAS SUMMARY (Dense RAG) ===\")\n",
    "        print(ablate_df.to_string(index=False))\n",
    "        print(\"Saved:\", out_summary)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"[RAGAS WARNING] Skipped due to error:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3532e5-cb9a-45e8-8d52-393bef0b33a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
